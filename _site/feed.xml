<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-06-22T21:20:30-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sebastian Yang</title><subtitle>This is the personal website for Sebastian Yang aka 杨毓恺.</subtitle><entry><title type="html">Tiktok in AR/VR</title><link href="http://localhost:4000/blogs/2023/summer/Reimagine_TikTok" rel="alternate" type="text/html" title="Tiktok in AR/VR" /><published>2023-06-10T07:40:23-07:00</published><updated>2023-06-10T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2023/summer/Reimagine_TikTok</id><content type="html" xml:base="http://localhost:4000/blogs/2023/summer/Reimagine_TikTok">&lt;h2 id=&quot;introduction&quot;&gt;Introduction&lt;/h2&gt;

&lt;p&gt;Welcome to the “Reimagine” series. In this series, I’ll pick from the Top 100 apps in the app stores and reimagine them as AR/VR apps in three-dimensional space.&lt;/p&gt;

&lt;p&gt;This article will explore TikTok. The simplest approach would be to project the 2D TikTok interface into a big screen in AR glasses for the user. This kind of application might appear in the next few years. However, I hope to break away from the two-dimensional space and re-examine TikTok from a three-dimensional perspective.&lt;/p&gt;

&lt;p&gt;For this imaginative exercise, let’s assume we have the perfect pair of AR glasses at our disposal. Our exploration will delve into three parts:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;What problem is it solving?&lt;/li&gt;
  &lt;li&gt;Why can AR/VR solve this problem better?&lt;/li&gt;
  &lt;li&gt;What would the 3D user experience be like?&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;what-problem-is-it-solving&quot;&gt;What problem is it solving?&lt;/h2&gt;

&lt;p&gt;We can broadly categorize TikTok users into content creators and content viewers.&lt;/p&gt;

&lt;h3 id=&quot;viewers&quot;&gt;Viewers&lt;/h3&gt;
&lt;p&gt;Problem: Alleviating boredom&lt;/p&gt;

&lt;p&gt;Solution: By providing short video content, users can easily discover and filter content they are interested in for instant entertainment and enjoyment.&lt;/p&gt;

&lt;p&gt;Depending on the use case, we can further divide viewers into fragmentary time users and focused time users.&lt;/p&gt;

&lt;p&gt;Fragmentary time users: These users browse TikTok during short pockets of time in their life (like waiting for a bus, queuing, etc.). Notably, in these scenarios, users need to stay aware of their surroundings, for example, noticing when the bus arrives, whether they need to move forward, etc.&lt;/p&gt;

&lt;p&gt;Focused time users: These users browse TikTok during large blocks of free time (like before sleep, leisurely weekends, etc.). In these scenarios, users are typically in a comfortable environment, without the need to pay attention to changes around them. They can fully invest in consuming TikTok content.&lt;/p&gt;

&lt;h3 id=&quot;creators&quot;&gt;Creators&lt;/h3&gt;
&lt;p&gt;Problem: High cost of making long videos and the content’s low likelihood of being discovered by users&lt;/p&gt;

&lt;p&gt;Solution: By adopting the short video format, the cost of producing high-quality videos is reduced, and a special ranking algorithm increases the chances of videos becoming viral.&lt;/p&gt;

&lt;p&gt;Creators can be further divided into UGC (User Generated Content) creators and PGC (Professionally Generated Content) creators.&lt;/p&gt;

&lt;p&gt;UGC creators: These creators typically use the built-in camera on their phones to record everyday life, shoot dances, capture moments, or create interesting content with filters. Their main need is to be able to record life instantaneously and conveniently and to edit videos simply and quickly.&lt;/p&gt;

&lt;p&gt;PGC creators: They typically use professional microphones and cameras and may rely on external professional software for video editing, script design, and graphic design. Their main need is to produce high-quality content more conveniently and hope this content can be efficiently distributed to users through the platform.&lt;/p&gt;

&lt;p&gt;While TikTok has many other uses, such as live streaming e-commerce, brand promotion, AI-generated content, etc., due to space limitations, I’ll mainly focus on the four use cases mentioned above. One of TikTok’s strengths is its accurate recommendation algorithm, but since it is “invisible” backend work and unrelated to the user’s direct experience, this article will focus on aspects other than the recommendation algorithm.&lt;/p&gt;

&lt;h2 id=&quot;why-can-arvr-solve-these-problems-better&quot;&gt;Why can AR/VR solve these problems better?&lt;/h2&gt;

&lt;h3 id=&quot;meeting-the-demand-for-immersive-experiences&quot;&gt;Meeting the demand for immersive experiences&lt;/h3&gt;
&lt;p&gt;In the current 2D social media, users can only consume content through text, images, and videos. While these methods can effectively convey information, it can’t compete with 3D experiences in terms of immersion and authenticity.&lt;/p&gt;

&lt;p&gt;In the era of spatial computing, we can display three-dimensional models, 3D images and videos with depth perception, and even 3D scenes that can completely replace a user’s reality. These new forms of media won’t replace the old ones but will coexist with them, enriching our media ecosystem.&lt;/p&gt;

&lt;h3 id=&quot;breaking-the-limitations-of-interactivity&quot;&gt;Breaking the limitations of interactivity&lt;/h3&gt;
&lt;p&gt;The interactivity of 2D social media mainly takes the form of likes, repost, comments,  etc. However, in a 3D AR/VR environment, users can interact with content and other users in more intuitive and rich ways, such as moving directly in the 3D environment, manipulating objects, gesturing, etc.&lt;/p&gt;

&lt;h3 id=&quot;unlocking-the-space-for-creativity&quot;&gt;Unlocking the space for creativity&lt;/h3&gt;
&lt;p&gt;In a 2D environment, creators are limited to expressing themselves within a two-dimensional plane. However, in a 3D environment, they can create richer, more dimensional content. &lt;strong&gt;This means that TikTok could transition from a “short video” platform to a “short-form experiences” platform.&lt;/strong&gt;&lt;/p&gt;

&lt;p&gt;Creators will no longer be restricted to a fixed-size screen but will be able to create in a three-dimensional space. They can choose to only display 2D content in this space, mix 2D and 3D content, or completely showcase pure 3D scenes. In pure 3D scenes, creators can also choose whether to completely replace the user’s reality with virtual content or intertwine virtual content into the user’s reality.&lt;/p&gt;

&lt;h3 id=&quot;in-conclusion&quot;&gt;In conclusion&lt;/h3&gt;
&lt;p&gt;For content consumers, the existing TikTok already offers instant fun by combining video and music. However, in the future, TikTok can transport users into “short experiences” through various virtual scenes, providing unprecedented dopamine stimulation.&lt;/p&gt;

&lt;p&gt;For content creators, this three-dimensional creative space will attract more game developers and modelers to participate. The cost of creating “short experiences” will be much lower than developing a full game, undoubtedly sparking more innovation and creativity.&lt;/p&gt;

&lt;h2 id=&quot;what-would-the-user-experience-be-like&quot;&gt;What would the user experience be like?&lt;/h2&gt;

&lt;p&gt;Riker sat alone in the airport terminal, like an island in a sea of bustling travelers. He casually put on his AR glasses, selected the TikTok app, plunging into its mixed reality mode. He could see his surrandings as before but he just stepped into a new universe.&lt;/p&gt;

&lt;p&gt;In the narrow terminal aisle in front of Riker, two vivid virtual hip-hop dancers appeared, performing the latest dancing challenge.They may have been captured as 3D video thousands of miles away, yet their movements and smiles were rendered as vividly as if they were dancing in front of him. In this modern age, creating 3D videos and photos with AR glasses was as straightforward as shooting 2D videos with a smartphone. As the dancers grooved, 3D heart emojis were floating upwards behind them. Suddenly, one dancer slipped, faltering towards a passenger sitting opposite Riker. But the passenger remained oblivious, for the spectacle unfolded solely within Riker’s AR glasses. Riker let out a slight chuckle. Although he wasn’t a dance enthusiast, this novel experience still intrigued him. He formed his hand to a thumbs-up gesture. It was instantly recognized by the AR glasses, and one more heart emoji rose slowly behind the dancers.&lt;/p&gt;

&lt;p&gt;With a light swipe on his thigh, the next experience appeared before Riker’s eyes. It was a financial news segment. The news anchor, dressed in a blue suit, seemed to be standing in real life in front of Riker’s left side. To Riker’s right, a 2D stock price chart presented the falling shares of a sports shoe company, which later animated into a 3D chart that introduced a new parameter -— user complaint numbers. As the anchor narrated the company’s spiking complaint numbers, Riker was interacting with the 3D chart, exploring it from various angles.  At the end of the video, the chart morphed into the company’s logo, and a virtual basket of fresh tomatoes and a basket of roses appeared before Riker. The anchor encouraged viewers to vote whether they would continue to purchase the company’s sport shoes. Watching other tomatoes flying towards the company’s logo, Riker picked up a virtual tomato and threw it too. In this era, creating such 3D interactive experiences has become incredibly simple. A slew of entry-level 3D scene design software sprang up, allowing many creators to effortlessly make 3D interactive content.&lt;/p&gt;

&lt;p&gt;The boarding announcement echoed through the terminal. With a swift gesture, Riker confined the TikTok app to a space in his immediate right before rising to join the boarding queue. Standing in line, he could enjoy TikTok on his right while keeping an eye on the movement of the queue ahead.&lt;/p&gt;

&lt;p&gt;Having found his seat on the plane and fastened his seatbelt, Riker switched TikTok to immersive mode. In this mode, his surroundings were completely replaced by virtual environment, as if he had been teleported to another space. The first thing he saw was a cute golden retriever rolling on a sofa in front of him. This was a 3D video, and Riker felt as if he were sitting in that family’s living room, as if he could reach out and touch the fluffy belly of the dog. If this was in mixed reality mode, TikTok would automatically remove the living room background and only place the golden retriever in Riker’s real environment. But in immersive mode, Riker felt as if he had been transported directly into that family’s living room, totally immersed in their happy moments with the dog.&lt;/p&gt;

&lt;p&gt;In the next experience, a huge pair of sports shoes appeared before Riker. Against the deep black background, the shoes were spinning elegantly like a prized exhibit. Accompanied by a powerful narrative voice, Riker was guided to look at his feet and try the shoes in AR – it was a 3D advertisement. Noticing Riker didn’t try the AR fitting, virtual gestures appeared on both sides of the shoes indicating it could be interacted with. Curiously, Riker pinched and drag the shoes to magnify them, revealing their stunningly detailed textures. Riker thought the shoe looked familiar, it seemed to be the same pair the dancers in the previous video was wearing when they slipped. Without thinking deeper, Riker swiped his finger lightly, moving on to the next experience.&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;h2 id=&quot;final-thoughts&quot;&gt;Final thoughts&lt;/h2&gt;

&lt;p&gt;It’s worth mentioning that in today’s internet environment, due to bandwidth limitations, real-time downloading of large 3D scene files to create a smooth feed still presents challenges. But with the advent of the 5G era, everything could change. Under the speedy 5G network, real-time downloads of 10-20MB or even larger 3D scenes will become a breeze, just as it was once unimaginable for TikTok to play 5-6MB short videos smoothly in a 3G network environment.&lt;/p&gt;

&lt;p&gt;At the same time, 3D models and content will become as common and easy to obtain as pictures and videos today. In the future, creating a 3D model will be as simple as taking a photo - in fact, we can now create 3D models by scanning objects with a phone, a technology that will only become more convenient and popular in the future.&lt;/p&gt;

&lt;p&gt;Similarly, creating 3D experiences will be as easy to get started as video editing but with a high ceiling. Users can easily layout 3D items, animations, 3D videos, 2D pictures or videos with an editor, and control the order of play, spatial position, and size changes by adjusting the timeline.&lt;/p&gt;

&lt;p&gt;Going one step further, recording 3D videos or taking 3D photos will be as easy as creating 2D content. Once AR glasses become widespread, anyone can easily record life’s wonderful moments in 3D. It’s a new world filled with infinite possibilities and imagination, and we look forward to its arrival.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;I want to reiterate that all the views I share here are my own. These thoughts do not represent the positions of my company or employer, nor do they contain any confidential information. My sole aim is to stimulate imagination and enthusiasm for potential AR/VR applications, helping everyone better understand this new field full of unknowns and possibilities through my personal insights and observations.&lt;/p&gt;

&lt;p&gt;I am fully aware that although I’ve made thoughtful explorations, with technological advancements and the passage of time, my depiction of the future might prove incorrect. Looking back from the future, many of my views may seem naive. But that won’t dampen my passion and interest in this exploration. I believe that only through trial and error can we discover new possibilities and innovate better solutions.&lt;/p&gt;

&lt;p&gt;Finally, I hope this series will inspire you, and I also look forward to your sharing of ideas and observations.&lt;/p&gt;</content><author><name></name></author><summary type="html">Introduction</summary></entry><entry><title type="html">Reimagine Series Prologue</title><link href="http://localhost:4000/blogs/2023/summer/Reimagine_Prologue" rel="alternate" type="text/html" title="Reimagine Series Prologue" /><published>2023-06-09T07:40:23-07:00</published><updated>2023-06-09T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2023/summer/Reimagine_Prologue</id><content type="html" xml:base="http://localhost:4000/blogs/2023/summer/Reimagine_Prologue">&lt;h2 id=&quot;what-is-this-series-about&quot;&gt;What is this series about?&lt;/h2&gt;

&lt;p&gt;Welcome to the “Reimagine” series. In this series, I will select applications from the Top 100 in the app store and reimagine what they would be like in a three-dimensional space, as AR/VR software. We will no longer be confined to the two-dimensional interfaces of mobile phones and computers, but shift our focus towards the more tangible and realistic three-dimensional space provided by AR/VR.&lt;/p&gt;

&lt;h2 id=&quot;why-do-i-write-this-series&quot;&gt;Why do I write this series?&lt;/h2&gt;

&lt;p&gt;We can make two basic assumptions about the future of technology:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;People desire more natural and realistic ways of interaction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Looking back over the past decade or so, it’s clear to see that from the text messages and emails of the 2G era to the pictures and voice messages of the 3G era, and then to the short videos and live broadcasts of the 4G era, the carriers of information flow are becoming increasingly enriched, presenting in ways that are closer to the real world.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;As three-dimensional beings, humans naturally understand and accept three-dimensional content.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Our everyday lives take place in three-dimensional space. In other words, we are inherently accustomed to interacting in three dimensions, not confined to two-dimensional screens.&lt;/p&gt;

&lt;p&gt;Based on these two assumptions, we have reason to believe that spatial computing brought about by AR/VR will be the next growth point in human technology.&lt;/p&gt;

&lt;p&gt;However, I found that many people’s thinking is still confined to a two-dimensional plane, and it’s hard to imagine what WeChat or Taobao would be like in three dimensions. Yet, the software on our phones satisfies various daily needs, which will still exist in the era of spatial computing. This implies that these apps on our phones need to be redesigned and implemented in a three-dimensional environment.&lt;/p&gt;

&lt;p&gt;Through this series, I hope to explore with you the possibilities of this new world and envision the various stunning applications that may appear in the future.&lt;/p&gt;

&lt;h2 id=&quot;how-will-i-appraoch-it&quot;&gt;How will I appraoch it?&lt;/h2&gt;

&lt;p&gt;To make this series more meaningful, I’ll undertake the following steps in my exploration:&lt;/p&gt;

&lt;p&gt;Firstly, we need to set up an ideal environment. Assume we already have an ideal AR headset: a comprehensive, smooth, and convenient device that seamlessly integrates virtual information into the real world, providing users with unprecedented interactive experiences.&lt;/p&gt;

&lt;p&gt;Under this premise, I’ll select a series of killer apps from the app store based on their ranking. These apps may include social, shopping, education, etc., all of which are favored by a broad user base due to their effectiveness in solving certain user needs.&lt;/p&gt;

&lt;p&gt;Then, I’ll apply the following three-step analysis to these apps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;What problem is it solving?&lt;/li&gt;
  &lt;li&gt;Why can AR/VR solve this problem better?&lt;/li&gt;
  &lt;li&gt;What will the specific user experience be like?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through these steps, I hope to spark our thinking about spatial computing and open our imagination to future technologies.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;At the start of this journey, I want to make it clear: all the views I share here are my own. These thoughts do not represent the positions of my company or employer, nor do they contain any confidential information. My sole aim is to stimulate imagination and enthusiasm for potential AR/VR applications, helping everyone better understand this new field full of unknowns and possibilities through my personal insights and observations.&lt;/p&gt;

&lt;p&gt;I am fully aware that although I’ve made thoughtful explorations, with technological advancements and the passage of time, my depiction of the future might prove incorrect. Looking back from the future, many of my views may seem naive. But that won’t dampen my passion and interest in this exploration. I believe that only through trial and error can we discover new possibilities and innovate better solutions.&lt;/p&gt;

&lt;p&gt;Finally, I hope this series will inspire you, and I also look forward to your sharing of ideas and observations.&lt;/p&gt;</content><author><name></name></author><summary type="html">What is this series about?</summary></entry><entry><title type="html">Vrlingo</title><link href="http://localhost:4000/projects/2022/fall/Vrlingo" rel="alternate" type="text/html" title="Vrlingo" /><published>2023-01-13T21:40:23-08:00</published><updated>2023-01-13T21:40:23-08:00</updated><id>http://localhost:4000/projects/2022/fall/Vrlingo</id><content type="html" xml:base="http://localhost:4000/projects/2022/fall/Vrlingo">&lt;p&gt;Have you ever tried speaking a new language? It’s hard right? To make it worse, there’s few opportunities to practice. You need to find a fluent speaker to practice with or spend a lot of money to hire a tutor. That’s why we’re dedicated to providing a platform for everyone to practice speaking new languages for free.&lt;/p&gt;

&lt;p&gt;We are a language learning community to help you practice with other people in virtual worlds. We have introduced a credit system, called TBucks, which allows you to gain credits by helping others practice speaking in your native language. You can then use those credits to practice a new language you want to learn.&lt;/p&gt;

&lt;p&gt;You can try it out &lt;a href=&quot;https://github.com/SCP650/Vrlingo-New&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/mgdhqsrxNBY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;free-language-practice&quot;&gt;Free Language Practice&lt;/h2&gt;

&lt;p&gt;We strive to create a community where each user not only learns a new language, but also acts as a teacher to assist others with their native language. After specifying the languages they know and wish to learn, we provide a live map of all current instances where those specified languages are being spoken. Users can then choose to join as a teacher to earn TBucks or as a student to spend them. This fosters a language learning community and allows for free practice opportunities for speaking.&lt;/p&gt;

&lt;h2 id=&quot;why-vr&quot;&gt;Why VR?&lt;/h2&gt;

&lt;p&gt;VR allows us to build situations and an immersive environment for role-playing.&lt;/p&gt;

&lt;p&gt;For example, we built a restaurant world with many interactive props. We noticed that many Mandarin learners will pretend to order food, talk about their favorite dishes, or try to rob the restaurant while speaking in Mandarin. This makes the language speaking fun and mimics real-life situations.&lt;/p&gt;

&lt;p&gt;To take it a step further, we made contextually-driven prompts that provide topics to talk about. For example, when the user picks up some money in the VR restaurant, they will see “Buy some food with money.” This invites a conversation between the user holding the money and other users in the VR restaurant.&lt;/p&gt;

&lt;h2 id=&quot;monetization&quot;&gt;Monetization&lt;/h2&gt;

&lt;p&gt;There are three ways we could monetize in the future.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ads: We can place immersive ads in the VR world. For example, a branded soda in the restaurant world, or make branded coffee shop as a world for users to practice speaking in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Professional Tutors: Some users may appreciate professional tutors such as 1:1 lessons in addition to the free community support. Hence, we can offer classes and take a percentage of the fee.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Subscriptions: We can offer a monthly subscription where users can gain additional features such as show hints to pronounce when grabbing objects, special status icons, and remove ads.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;We use Unity and XR Interaction Toolkit to create the app. The multiplayer networking, VOIP, synced variables and avatar sync are created using Normcore.&lt;/p&gt;

&lt;p&gt;We made this app in 2 days and won the Best Shared World Experience Award at MIT Reality Hack!&lt;/p&gt;

&lt;p&gt;PS: We created the icon of this page using Stable Diffusion.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><category term="Product_Project_Management" /><summary type="html">Have you ever tried speaking a new language? It’s hard right? To make it worse, there’s few opportunities to practice. You need to find a fluent speaker to practice with or spend a lot of money to hire a tutor. That’s why we’re dedicated to providing a platform for everyone to practice speaking new languages for free.</summary></entry><entry><title type="html">Match-3 Siege</title><link href="http://localhost:4000/projects/2022/winter/Match3Shooter" rel="alternate" type="text/html" title="Match-3 Siege" /><published>2022-12-30T01:40:23-08:00</published><updated>2022-12-30T01:40:23-08:00</updated><id>http://localhost:4000/projects/2022/winter/Match3Shooter</id><content type="html" xml:base="http://localhost:4000/projects/2022/winter/Match3Shooter">&lt;p&gt;What would Candy Crush + Call of Duty look like?&lt;/p&gt;

&lt;p&gt;Many innovations in video games come from blending genres – what happens if we combine the most popular genre on mobile (match-3) with the most popular genre on PC (shooter)?&lt;/p&gt;

&lt;p&gt;Introducing Match-3 Siege! You can download the demo &lt;a href=&quot;https://github.com/SCP650/Match3Shooter-Unreal5/releases/tag/v1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;In this game, you’ll be part of either the attacking or defending team. The attackers must destroy all of the colored spheres on the field, while the defenders must protect them.&lt;/p&gt;

&lt;p&gt;To destroy the spheres, attackers must shoot and match three or more of the same color, while fighting against the defenders’ fire.&lt;/p&gt;

&lt;p&gt;To defend the spheres, the defenders need to find and guard strategic positions to prevent match-3 combinations from occurring (e.g. blue blue red blue) and insert new spheres into the field every 30 seconds to make it harder for the attackers (e.g. blue yellow blue red blue).&lt;/p&gt;

&lt;p&gt;If the attackers succeed in destroying all the spheres before time runs out, they win. If any spheres remain, the defenders win.&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/saNxsox4Wz8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The video above is a single-player demo I made for this idea. Although it’s against AI, you can already feel the intense and fast-paced adrenaline rush caused by constantly switching between playing match-3 and fighting against enemies.&lt;/p&gt;

&lt;p&gt;I think this game mode could be added alongside team deathmatch, search and destroy, and battle royal. Let me know what you think!&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This game was made independently by me using Unreal Engine 5.1 and C++. It is my first time making Unreal games with C++ and it has been a great learning experience. I used free 3D and animation assets and implemented the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Animation blending using blendspace&lt;/li&gt;
  &lt;li&gt;Input mapping for locomotion&lt;/li&gt;
  &lt;li&gt;AI behavior tree and blackboard for enemies&lt;/li&gt;
  &lt;li&gt;Integration of particles and sound for shooting&lt;/li&gt;
  &lt;li&gt;Lose and win conditions&lt;/li&gt;
  &lt;li&gt;Match-3 system with chaos destruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PS: I created the icon of this page using Stable Diffusion and the text of this page using ChatGPT&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="Game_Development" /><summary type="html">What would Candy Crush + Call of Duty look like?</summary></entry><entry><title type="html">A Second Person VR Game</title><link href="http://localhost:4000/projects/2022/fall/SecondPerson" rel="alternate" type="text/html" title="A Second Person VR Game" /><published>2022-11-26T21:40:23-08:00</published><updated>2022-11-26T21:40:23-08:00</updated><id>http://localhost:4000/projects/2022/fall/SecondPerson</id><content type="html" xml:base="http://localhost:4000/projects/2022/fall/SecondPerson">&lt;p&gt;Have you ever played a second-person multiplayer VR shooting game? Me neither, so I made one over the Thanksgiving break.&lt;/p&gt;

&lt;p&gt;Well, we first need to figure out what is a second-person game.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In a first-person game, you control yourself through your own perspective – I’m going left, I’m going right.&lt;/li&gt;
  &lt;li&gt;In a third-person game, you control yourself through an external perspective – They go left, they go right.&lt;/li&gt;
  &lt;li&gt;So naturally, in a second-person game, you control yourself through the perspective of another character – You go left, you go right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this game, there are two players and two guns on the table. The goal is very simple: whoever picks up the gun and shoots the other player first wins. But there’s one catch: it’s a second-person game – you are controlling yourself through the perspective of your opponent.&lt;/p&gt;

&lt;p&gt;What does that actually mean? In this clip, it may look like a normal first-person game but you are not controlling yourself – if you raise your VR controller, the character in front of you will raise their hand, not the character you are in the perspective of.&lt;/p&gt;

&lt;p&gt;Originally I thought this would be as if I’m controlling myself through a mirror, but in reality, it is much worse …&lt;/p&gt;

&lt;p&gt;You can try it out &lt;a href=&quot;https://github.com/SCP650/SecondPersonVRGame/releases/tag/v1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/fCkHKXO1GiU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-true-second-person-vr-games-are-bad&quot;&gt;1. True Second Person VR Games are BAD&lt;/h3&gt;

&lt;p&gt;To make a true second-person game meant you cannot control the camera, since the camera is technically on the head of another character who you don’t control – you just happen to look out from their perspective.&lt;/p&gt;

&lt;p&gt;What that means for VR is that when you move your head to look right, the view you are seeing is not moving, instead, you have to ask the other player to move their head to look right so that you can see what’s on the right.&lt;/p&gt;

&lt;p&gt;This also means if the other player moves their head randomly, their view is not going to be impacted (since your head is not moving), but your view will move randomly causing vertigo and dizziness.&lt;/p&gt;

&lt;h3 id=&quot;2-a-compatitive-game-become-cooperative-quickly&quot;&gt;2. A Compatitive Game Become Cooperative Quickly&lt;/h3&gt;

&lt;p&gt;Even though the goal of the game is to kill the other player, we quickly realize it’s not playable if we don’t cooperate.&lt;/p&gt;

&lt;p&gt;In the video, you can see we struggle to get our own character inside our view. This is because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Player A can only see stuff from Player B’s perspective. So A needs to give B instructions to adjust A’s view.&lt;/li&gt;
  &lt;li&gt;But B doesn’t know what A is seeing (B can only see A’s perspective), so B doesn’t know many degrees they need to turn.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hence we end up spending the majority of the play time telling the other party how to move their head – “Can you look right?”,”Can you look left?” – which I guess makes this a successful second-person game..? but not an enjoyable game.&lt;/p&gt;

&lt;h3 id=&quot;3-player-will-instinctively-move-their-head-to-adjust-the-camera&quot;&gt;3. Player Will Instinctively Move Their Head to Adjust the Camera&lt;/h3&gt;

&lt;p&gt;One added difficulty I wasn’t expecting before was that when A wants to look left, A will not only give instructions to B to look left but also instinctively move their head to the left – this means it’s also disrupting B’s view.&lt;/p&gt;

&lt;p&gt;And then B’s view is disrupted, B will instinctively move their head in the opposite direction, further disrupting A’s view. This would create a compound effect what gets worse very quickly…&lt;/p&gt;

&lt;h2 id=&quot;some-thoughts&quot;&gt;Some Thoughts…&lt;/h2&gt;

&lt;p&gt;I started this project knowing a second-person VR game would be a bad idea. But my main goal is to figure out how to network a VR game, and I did end up with a multiplayer VR game so I consider this a success!&lt;/p&gt;

&lt;p&gt;Nonetheless, let this serve as a lesson to whoever comes after me that wants to make a second-person VR game. Unless you want to make major sacrifices (like in Trover Saves the Universe), a true second-person game is not a good idea.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with &lt;a href=&quot;https://assetstore.unity.com/packages/3d/props/interior/polygon-dining-room-199435&quot;&gt;free 3D assets&lt;/a&gt; from Asset Store, the Oculus Plugin, and the BNG Interaction Framework. It is networked using Normcore. This app is made in Unity with Oculus Quest 2. It can also run on a windows machine with Oculus Link.&lt;/p&gt;

&lt;p&gt;PS: I created the icon of this page using text to image AI.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever played a second-person multiplayer VR shooting game? Me neither, so I made one over the Thanksgiving break.</summary></entry><entry><title type="html">Earf</title><link href="http://localhost:4000/projects/2022/summer/earf" rel="alternate" type="text/html" title="Earf" /><published>2022-08-20T22:40:23-07:00</published><updated>2022-08-20T22:40:23-07:00</updated><id>http://localhost:4000/projects/2022/summer/earf</id><content type="html" xml:base="http://localhost:4000/projects/2022/summer/earf">&lt;p&gt;Enjoying pictures of Earth? Why not try Earf – the AR Planet Generator!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tap on the screen to generate gorgeous planets&lt;/li&gt;
  &lt;li&gt;Move your device around to view them up close&lt;/li&gt;
  &lt;li&gt;Simulate physics to see planets collide!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To download the full app: &lt;a href=&quot;https://apps.apple.com/us/app/earf-ar-planets/id1641207075&quot;&gt;Apple App Store&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/UuTJdkB-cac&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This app is made with Unity using AR Foundation. Each terrains is procedurally genearted using multiple layers of perlin noise. A custom shader is made to coloring the planets with random gradients based on the distance of each vertex to the center. It simulations physically (sorta) accurate gravitational force. And also has a random name generator to give your pet planets names!&lt;/p&gt;

&lt;h2 id=&quot;how-was-it-made&quot;&gt;How was it made?&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p1.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p1.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Starting with a blob&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p2.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p2.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Make it round&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p3.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p3.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Sprinkle some perlin noise&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p4.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p4.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Clamp the noise to make ocean surface&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p5.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p5.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the mountains with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p6.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p6.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the planet with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p7.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p7.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the ocean depths with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p8.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p8.png&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Done!&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><summary type="html">Enjoying pictures of Earth? Why not try Earf – the AR Planet Generator!</summary></entry><entry><title type="html">Siggraph 2022 Day 2</title><link href="http://localhost:4000/blogs/2022/summer/Siggraph22_Part2" rel="alternate" type="text/html" title="Siggraph 2022 Day 2" /><published>2022-08-12T07:40:23-07:00</published><updated>2022-08-12T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2022/summer/Siggraph22_Part2</id><content type="html" xml:base="http://localhost:4000/blogs/2022/summer/Siggraph22_Part2">&lt;p&gt;Hi all, welcome to my Siggraph series where I post the TLDR of the most memorable stuff I see at the conference.&lt;/p&gt;

&lt;p&gt;This started as some rambling notes I took during the conference, and I just cleaned it up a bit. It may not be the most accurate (especially the technical papers) but feel free to take a look!&lt;/p&gt;

&lt;h2 id=&quot;day-2-events-ive-attended&quot;&gt;Day 2 Events I’ve attended:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tech Papers: &lt;strong&gt;Ray Tracing &amp;amp; Monte Carlo Methods&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: &lt;strong&gt;Emerging Technologies&lt;/strong&gt; + &lt;strong&gt;Immersive Pavilion&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Presentation: &lt;strong&gt;Advances in Real-Time Rendering in Games: Part II&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: Companies&lt;/li&gt;
  &lt;li&gt;Unreal Talk: &lt;strong&gt;Animating In-Engine - Real-Time Production Workflows&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: Appy Hour&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;highlights&quot;&gt;Highlights&lt;/h2&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;immersive-pavilion&quot;&gt;Immersive Pavilion&lt;/h2&gt;

&lt;h3 id=&quot;a-vr-locomotion-method-that-kinda-solves-motion-sickness&quot;&gt;A VR Locomotion Method that (kinda) Solves Motion Sickness&lt;/h3&gt;

&lt;p&gt;What is it: The demo is called “&lt;strong&gt;HyperJumping&lt;/strong&gt;” by Bernhard E. Riecke. It has two parts: lean based locomotion and hyperjump.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lean-Based Locomotion&lt;/strong&gt;: instead of pushing a thumbstick, you have to physically lean forward to move forward. Since your body is actually leaning forward as your visual changes, it’s less dizzy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hyperjump&lt;/strong&gt;: when the user reached a max speed, it will automatically teleport the user for a segment, and then the user will continue to travel at max speed.&lt;/p&gt;

&lt;p&gt;Use case: if a person wants to travel long distances in VR, they don’t want to use smooth locomotion (which is dizzy) and don’t want to teleport (since they will miss the view), they will use lean-based movement + hyperjump.&lt;/p&gt;

&lt;p&gt;Sebastian’s Review: conceptually this is a very interesting idea — combining physical movement with smooth locomotion and teleportation, but in practice, when I tried the demo the Hyperjump(teleportation) feels scary — I would suddenly teleport to another place without any warning. I really like the idea and think further exploration is required.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;expo-companies&quot;&gt;Expo: Companies&lt;/h2&gt;

&lt;p&gt;I tried a ton of demos at the expo, these are the top most memorable three&lt;/p&gt;

&lt;h3 id=&quot;omniverse&quot;&gt;Omniverse&lt;/h3&gt;

&lt;p&gt;I was really really shocked when I saw a live demo of Nvidia’s Omniverse. The demo showcases a game development workflow of 3 people, using Maya, Unreal and Adobe Substance 3D respectively on their own laptops.&lt;/p&gt;

&lt;p&gt;With Omniverse, everything is synced in real-time bi-directionally: if a new car is added in Unreal’s scene, it will be reflected in Maya and Substance 3D instantly. Then the artist in Substance 3D will pain the car, the texture will show up instantly in Unreal and Maya.&lt;/p&gt;

&lt;p&gt;This workflow is very magical and can drastically increase a team’s collaboration efficiency. Under the hood, everything works because they are using Disney’s new and open Universal Scene Description (USD) format. 
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/siggraph22/omniverse.jpg&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/siggraph22/omniverse.jpg&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Omniverse Integarting 3 Different Workflow&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;h3 id=&quot;oppo-air-glass&quot;&gt;Oppo Air glass&lt;/h3&gt;

&lt;p&gt;I tried Oppo’s Air glass, which is an extension screen you can attach to any glasses. It can display information like weather, messages and live translation.&lt;/p&gt;

&lt;p&gt;In theory, it sounds very good, but when I tried it in practice, I need to constantly change my eye focus between the glass and the person I’m talking to in real life which result in fatigue very quickly. The major issue is that the info is not spatialized. Having a screen that constantly displays irrelevant info is also distracting. It’s a good tech demo but would need a lot more improvement to be consume-ready.&lt;/p&gt;

&lt;h3 id=&quot;stretchsense-gloves&quot;&gt;StretchSense Gloves&lt;/h3&gt;

&lt;p&gt;A current problem with optical hand tracking is that if the hand is obstructed, it can’t be tracked. StretchSense gloves managed to imbed a lot of senses in a normal looking glove, and send rotational information from each joint to their software, which will then convert the data to Unity/Unreal supported format.&lt;/p&gt;

&lt;p&gt;On the update, the glove feels comfortable and does hand tracking very feel. But it comes at a hefty~$5k. It’s a good mocap solution and for research use, but probably not for consumer tech.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;animating-in-engine---real-time-production-workflows&quot;&gt;Animating In-Engine - Real-Time Production Workflows&lt;/h2&gt;

&lt;p&gt;The folks at Epic Games showcased how you can create animations in-engineer without the need for professional software like Maya.&lt;/p&gt;

&lt;p&gt;In the demo, the speaker is able to copy parts of one animation (jumping) and paste them to another animation (moving forward), the final result is a person hopping forward.&lt;/p&gt;

&lt;p&gt;This is very impressive in that programmers can just download a few free mocap assets, and cut and paste different portions to combine to the desired motions. This makes prototyping ideas and making short video clips so much faster.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;tech-papers-ray-tracing--monte-carlo-methods&quot;&gt;Tech Papers: Ray Tracing &amp;amp; Monte Carlo Methods&lt;/h2&gt;

&lt;h3 id=&quot;r2e2-low-latency-path-tracing-of-terabyte-scale-scenes-using-thousands-of-cloud-cpus&quot;&gt;R2E2: Low-latency Path Tracing of Terabyte-scale Scenes Using Thousands of Cloud CPUs&lt;/h3&gt;

&lt;p&gt;Problem: if you need to render terabyte-scale scenes, and you don’t happen to own a super computer… you are out of luck&lt;/p&gt;

&lt;p&gt;Solution: Why now rent thousands of CPUs to run at the same time? That’s what the author did, live, at the presentation — he rented thousands of AWS E3 instances and rendered a path-traced picture of a terabyte scene in ~1 mins.&lt;/p&gt;

&lt;h3 id=&quot;generalized-resampled-importance-sampling-foundations-of-restir&quot;&gt;Generalized Resampled Importance Sampling: Foundations of ReSTIR&lt;/h3&gt;

&lt;p&gt;Problem: Traditional ray tracing is very costly and slow, it also requires multiple passes to generate a good image.&lt;/p&gt;

&lt;p&gt;Context: The ReSTIR approach - it reuses samples from previous frames to improve path distributions across frames (temporal reuse) and resample between this and nearby pixels (spatial reuse)&lt;/p&gt;

&lt;p&gt;Solution: Through some cool math and magic that I don’t fully understand, the author generalized the ReSTIR approach to work on any domains without the need to know the exact PDF&lt;/p&gt;

&lt;h3 id=&quot;regression-based-monte-carlo-integration&quot;&gt;Regression-based Monte Carlo Integration&lt;/h3&gt;

&lt;p&gt;Problem: Solving integration is a hard but common problem in graphics, e.g. to light up one pixel properly, we need to calculate the integral of all the light paths connecting the pixel to the light source.&lt;/p&gt;

&lt;p&gt;Context: Currently we mostly use Monte Carlo Integration, which randomly samples the function and averages those values to estimate the integral. However, this is only an estimate and has errors.&lt;/p&gt;

&lt;p&gt;Solutions: the author proposes a new estimator using regression function + residual, this estimator is provable better than Monte Carlo. Because in the worst case, the regression will be a constant, which will produce the same result as Monte Carlo.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hi all, welcome to my Siggraph series where I post the TLDR of the most memorable stuff I see at the conference.</summary></entry><entry><title type="html">Lightly Heavy</title><link href="http://localhost:4000/projects/2022/spring/Heavy" rel="alternate" type="text/html" title="Lightly Heavy" /><published>2022-02-04T16:40:23-08:00</published><updated>2022-02-04T16:40:23-08:00</updated><id>http://localhost:4000/projects/2022/spring/Heavy</id><content type="html" xml:base="http://localhost:4000/projects/2022/spring/Heavy">&lt;p&gt;Have you ever wondered what it feels like to fly in space? Well, now you can. This virtual reality game allows you to simulate space combat like in the Ender’s Game anytime and anywhere.&lt;/p&gt;

&lt;p&gt;This app is built in 36 hours independently by Sebastian Yang with original music composed by William Ozeas. It is developed for HoyaHack and won &lt;strong&gt;the U.S. Space Force Award&lt;/strong&gt;. You can download it and try it out &lt;a href=&quot;https://github.com/SCP650/LightlyHeavy-UnityVR/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/VyIAN2M-e8o&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-lock-rotations-and-lower-the-speed&quot;&gt;1. Lock Rotations and Lower the Speed!&lt;/h3&gt;

&lt;p&gt;I always thought continuous movement and jumping up and down in VR are the dizziest. I was wrong. When I first develop the movement mechanics, I didn’t lock rotation meaning if the player used the booster on their right hand, instead of going straight forward, they will start spinning like crazy while moving forward.&lt;/p&gt;

&lt;p&gt;The first few seconds were fun - as the speed is still slow. But afterward, the dizziness became unbearable. The takeaway here is to lower the speed to avoid dizziness, especially for beginners.&lt;/p&gt;

&lt;h3 id=&quot;2-prioritize-before-acting&quot;&gt;2. Prioritize Before Acting&lt;/h3&gt;

&lt;p&gt;There are two major components to this project: the tutorial area (spaceship) and the combat zone (giant sphere). Most developers will probably start with the combat zone since it’s the bulk of the game. However, I decided to start with the tutorial area.&lt;/p&gt;

&lt;p&gt;This is because the combat zone is essentially the tutorial areas with procedural generation – both shared the same movement systems and enemies. If I don’t have time to develop a good pro-gen system, the player could still learn how to play the game and explore a single level - the spaceship. However, if I started with the combat zone and the pro-gen is not working, the player would not know what to do and killed by enemies instantly.&lt;/p&gt;

&lt;p&gt;Even though I managed to finish the pro-gen systems, this prioritization could’ve saved my project.&lt;/p&gt;

&lt;h3 id=&quot;3-its-dangerous-to-go-alone&quot;&gt;3. It’s Dangerous to Go Alone!&lt;/h3&gt;

&lt;p&gt;I intentionally made this a solo project because I want to test my ability. Even though I was happy with the result, it was not fun to do everything by myself. I would prefer the collaboration and delegation I did in &lt;a href=&quot;http://localhost:4000/projects/2021/fall/Haunted&quot;&gt;Haunted&lt;/a&gt;. If possible, don’t go alone!&lt;/p&gt;

&lt;h2 id=&quot;some-thoughts&quot;&gt;Some Thoughts…&lt;/h2&gt;

&lt;p&gt;I’m very proud of this project, especially with the execution. In 36 hours, I managed to develop a movement mechanic in zero-g, an extendable enemy class system, and a procedural generation system for levels. I also managed to design a tutorial area with 3D assets, a combat zone in Blender, and multiple GUI components. And everything was done in a virtual reality setting.&lt;/p&gt;

&lt;p&gt;This was by no means a perfect execution – the enemy AI needs a lot more work and there’s a lack of more enemy and weapon types. But I’m very pleased with what I ended up with - a replayable space combat VR game that is actually fun to play.&lt;/p&gt;

&lt;p&gt;Moreover, it also (kinda) fulfilled my childhood dream - going to space. The Ender’s Game is one of my favorite novel series and movies. Seeing the fight in a giant sphere in space really inspired and motivated me. (heck I even applied to many top aerospace engineering programs during college applications). This project intersects my high school passion in space with my college passion in the metaverse. So I’m really glad I got to do this.&lt;/p&gt;

&lt;p&gt;Finally, this project is also a testament to my growth. When I first started three years ago, I would’ve never thought I can pull something like this off. During the past three years, I’ve tried a lot, failed a lot, learned a lot, and kept repeating. It is truly pleasing to see oneself grow.&lt;/p&gt;

&lt;p&gt;Now that my college life is over, a new journey awaits.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with &lt;a href=&quot;https://assetstore.unity.com/packages/3d/environments/sci-fi/sci-fi-styled-modular-pack-82913&quot;&gt;free 3D assets&lt;/a&gt; from Asset Store, free skyboxes, the Oculus Plugin and the BNG Interaction Framework. This app is made in Unity with Oculus Quest 2. It can run on a windows machine with Oculus Link.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever wondered what it feels like to fly in space? Well, now you can. This virtual reality game allows you to simulate space combat like in the Ender’s Game anytime and anywhere.</summary></entry><entry><title type="html">Game Creation Society</title><link href="http://localhost:4000/projects/2021/fall/GCS" rel="alternate" type="text/html" title="Game Creation Society" /><published>2022-01-23T16:40:23-08:00</published><updated>2022-01-23T16:40:23-08:00</updated><id>http://localhost:4000/projects/2021/fall/GCS</id><content type="html" xml:base="http://localhost:4000/projects/2021/fall/GCS">&lt;p&gt;The Game Creation Society is a student-led game development club at Carnegie Mellon. I’m honored to be elected as President of the Game Creation Society in 2020. In the fall semester alone, we have 100+ active members making 9 games in parallel.&lt;/p&gt;

&lt;p&gt;I joined this club in my freshman year not knowing anything about game development. But my time at GCS has been exceptional, I learned so much, made so many games, and met so many cool people. Here are some notable accomplishments we achieved during my time as President.&lt;/p&gt;

&lt;h2 id=&quot;accomplishments&quot;&gt;Accomplishments&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Attendence&lt;/strong&gt;:We had a record turnout this year. At the start of the semester, we started with 250+ people attending our info session and booth showing interest to join and end up with 100+ people actually attending. By the end of the semester, we successfully released 8 games at Hunt Library.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VR&lt;/strong&gt;: We purchased our first VR headset as a club and made two VR games in the semester after.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unreal + Unity Stuco&lt;/strong&gt;: We offered the first Unreal Engine course at Carnegie Mellon and continued our existing Unity student-taught course.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developer Fund&lt;/strong&gt;: We started a fund with a  few hundred dollars to support our developers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GCS Library&lt;/strong&gt;: We started a collection of assets that can be shared with all our members.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: Our Discord server continues to grow, now we have almost 600 members on the server.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Industry Talks&lt;/strong&gt;: We continued to host great industry talks.
    &lt;ul&gt;
      &lt;li&gt;We hosted an info session with &lt;em&gt;Epic Games&lt;/em&gt; for internship opportunity&lt;/li&gt;
      &lt;li&gt;We have &lt;em&gt;Rockstar Games&lt;/em&gt; gave the opening speech at our Pitch Fair.&lt;/li&gt;
      &lt;li&gt;We also invited alumni to join us, including the Tech Lead at &lt;em&gt;Unity&lt;/em&gt;, the Manager of Production at &lt;em&gt;PlayStation Studios&lt;/em&gt;, and the Lead Character Artist at &lt;em&gt;Heart Machine&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Personally, during my three years at GCS, I’ve made six games including PC, Mobile and VR games.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
    &lt;a href=&quot;https://www.gamecreation.org&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
      
        &lt;img src=&quot; /images/projects/GCS/website.png&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Game Creation Society Website&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;h2 id=&quot;to-gcs-members&quot;&gt;To GCS Members&lt;/h2&gt;

&lt;p&gt;I’m very honored to lead the club in the past year. I want to thank all the execs, team leads, alumni, and most of all, our members for giving me such a great experience.  I’ve no doubt the incoming execs will lead the club to the next great chapter, especially with the help of each and every one of you!&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="Software_Engineer" /><category term="Game_Development" /><category term="Product_Project_Management" /><summary type="html">The Game Creation Society is a student-led game development club at Carnegie Mellon. I’m honored to be elected as President of the Game Creation Society in 2020. In the fall semester alone, we have 100+ active members making 9 games in parallel.</summary></entry><entry><title type="html">Bat and Monitors</title><link href="http://localhost:4000/projects/2022/spring/Monitors" rel="alternate" type="text/html" title="Bat and Monitors" /><published>2022-01-22T16:40:23-08:00</published><updated>2022-01-22T16:40:23-08:00</updated><id>http://localhost:4000/projects/2022/spring/Monitors</id><content type="html" xml:base="http://localhost:4000/projects/2022/spring/Monitors">&lt;p&gt;Have you ever felt exhausted after a long day of work? Wanting to find a safe and private place to let it all out? Well, now you can. This application allows you to release all your stress and anger whenever you want and wherever you are. This is a rage room in virtual reality that gives you a baseball bat, and a lot of monitors.&lt;/p&gt;

&lt;p&gt;This app is built in 23 hours independently by Sebastian Yang for µHacks and it is the &lt;strong&gt;2nd place winner&lt;/strong&gt;. You can download it &lt;a href=&quot;https://github.com/SCP650/RageRoom-UnityVR/releases&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/lErdRuNXWKY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-a-well-scoped-project-is-very-important-for-hackathons&quot;&gt;1. A Well-Scoped project is Very Important for Hackathons.&lt;/h3&gt;

&lt;p&gt;In my previous hackathons, my team and I always over-scope and end up staying up late or not finishing all we wanted to do. This time I pick a very small scope, I ended finished the majority of the content on Saturday morning and have an entire afternoon to play with procedural generation.&lt;/p&gt;

&lt;h3 id=&quot;2-cheat-the-eyes-with-sound-effects&quot;&gt;2. Cheat the Eyes with Sound Effects&lt;/h3&gt;

&lt;p&gt;To properly shatter an object, I’ll have to use Bender to cut up the meshes and textures into smaller pieces. That’s a lot of work considering the number of objects I have. Luckily, I notice that people care more about the moment they hit the object than the actual shattered pieces of objects – especially the pieces that are smaller.&lt;/p&gt;

&lt;p&gt;So I ended up using particle systems to “fake” shattering for smaller objects like cups and pen holders. Coupled with a satisfying sound effect, it managed to fool people into thinking they actually shattered it. But if they look closer, they will notice all the pieces are just cubes&lt;/p&gt;

&lt;h3 id=&quot;3-always-back-up&quot;&gt;3. Always Back Up&lt;/h3&gt;

&lt;p&gt;I tend to use GitHub to do version control, however, committing assets from the Unity Assets Store is against the guideline. So I have to gitignore all those assets to keep my Github repo public. During my development, I made a lot of extensions based on store assets – creating new prefabs, extending code, etc. I even edited some assets directly. However, these edits are not tracked by GitHub.&lt;/p&gt;

&lt;p&gt;I ended up accidentally deleting all the store assets when merging branches and staring at an empty scene halfway through development – luckily I recovered everything from the recycling bin and I have intermediate builds I could always submit.&lt;/p&gt;

&lt;p&gt;However, in the future, if I have to gitignore important assets. I should not only make intermediate builds but also make a local copy of the entire project at each milestone.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with Unity’s free 3D Snaps Prototype assets, free skyboxes, the Oculus Plugin and the BNG Interaction Framework. This app is made in Unity with Oculus Quest 2. It can run on a windows machine AND standalone in an Oculus Quest 2 headset without the need of a PC.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever felt exhausted after a long day of work? Wanting to find a safe and private place to let it all out? Well, now you can. This application allows you to release all your stress and anger whenever you want and wherever you are. This is a rage room in virtual reality that gives you a baseball bat, and a lot of monitors.</summary></entry></feed>