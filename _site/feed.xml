<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" ><generator uri="https://jekyllrb.com/" version="3.8.7">Jekyll</generator><link href="http://localhost:4000/feed.xml" rel="self" type="application/atom+xml" /><link href="http://localhost:4000/" rel="alternate" type="text/html" /><updated>2023-06-21T22:51:00-07:00</updated><id>http://localhost:4000/feed.xml</id><title type="html">Sebastian Yang</title><subtitle>This is the personal website for Sebastian Yang aka 杨毓恺.</subtitle><entry><title type="html">Tiktok in AR/VR</title><link href="http://localhost:4000/blogs/2023/summer/Reimagine_TikTok" rel="alternate" type="text/html" title="Tiktok in AR/VR" /><published>2023-06-10T07:40:23-07:00</published><updated>2023-06-10T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2023/summer/Reimagine_TikTok</id><content type="html" xml:base="http://localhost:4000/blogs/2023/summer/Reimagine_TikTok">&lt;h2 id=&quot;前言&quot;&gt;前言&lt;/h2&gt;

&lt;p&gt;欢迎大家阅读「重定义」系列。在这个系列中，我将选择应用商店排名前100的手机软件，重新思考它们作为AR/VR软件在三维空间中的表现形式。&lt;/p&gt;

&lt;p&gt;本文我们讲探讨TikTok，最简单的方法就是把2D的tiktok界面，在AR眼镜里变成一个大屏幕给用户看。这样的应用在未来几年可能就会出现。但我希望打破二维空间的框架，从三维角度重新审视TikTok&lt;/p&gt;

&lt;p&gt;设想我们已经拥有了一副理想化的AR眼镜，接下来我会从以下三个角度对此进行思考：&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;它解决了谁的什么问题？&lt;/li&gt;
  &lt;li&gt;为什么AR/VR能更好地解决这个问题？&lt;/li&gt;
  &lt;li&gt;具体的用户体验会是怎样的？&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;它是为谁解决了什么问题&quot;&gt;它是为谁解决了什么问题？&lt;/h2&gt;

&lt;p&gt;我们可以将TikTok的用户大体划分为内容创作者和内容观看者两类。&lt;/p&gt;

&lt;h3 id=&quot;观看者&quot;&gt;观看者&lt;/h3&gt;
&lt;p&gt;解决的问题： 缓解无聊感
解决方法： 通过提供短视频内容，用户能够轻松地发现和筛选他们感兴趣的内容，实现即时的娱乐和乐趣。&lt;/p&gt;

&lt;p&gt;视使用场景的不同，我们可以将观看者进一步划分为碎片时间用户和专注时间用户。&lt;/p&gt;

&lt;p&gt;碎片时间用户： 这类用户会在生活中的小段空闲时间（如等待公交、排队等候等）中浏览TikTok。值得注意的是，在这种场景下，用户需要对周围环境保持敏感，例如留意公交车是否到站、是否需要前移等。&lt;/p&gt;

&lt;p&gt;专注时间用户： 这类用户会在一整块的空闲时间（如睡前、周末闲暇等）中浏览TikTok。这种场景下，用户通常在一个舒适的环境中，无需关注周围的环境变化，他们可以全心投入于消费TikTok内容。&lt;/p&gt;

&lt;h3 id=&quot;创作者&quot;&gt;创作者&lt;/h3&gt;
&lt;p&gt;解决的问题： 制作长视频的成本较高，且内容被用户发现的可能性较低
解决方法： 通过提供短视频格式，降低高质量视频的制作成本，同时采用特殊的排序算法以提升视频成为爆款的机会。&lt;/p&gt;

&lt;p&gt;创作者可以进一步划分为UGC（用户生成内容）创作者和PGC（专业生成内容）创作者。&lt;/p&gt;

&lt;p&gt;UGC创作者： 这类创作者通常使用手机自带的相机记录生活、拍摄舞蹈、捕捉生动瞬间或使用滤镜创作有趣的内容。他们的主要需求是能够即时便捷地记录生活，并能简单快速地进行视频剪辑。&lt;/p&gt;

&lt;p&gt;PGC创作者： 他们通常使用专业的麦克风和摄像头，借助外部软件进行视频剪辑，设计脚本和配图。他们的主要需求是能更方便地制作出高质量的内容，并希望这些内容能通过平台有效地分发给用户。&lt;/p&gt;

&lt;p&gt;虽然TikTok还有很多细分的使用场景，如直播电商、品牌宣传、AI生成的内容等，但由于篇幅原因，我将主要关注上述四类使用场景。TikTok的一个强大之处在于其精确的推荐算法，但由于这是“隐形”的后台工作，并且与用户的直接体验无关，因此本文将重点关注推荐算法之外的方面。&lt;/p&gt;

&lt;h2 id=&quot;arvr-如何更好地解决现有问题&quot;&gt;AR/VR 如何更好地解决现有问题？&lt;/h2&gt;

&lt;h3 id=&quot;满足沉浸式体验的需求&quot;&gt;满足沉浸式体验的需求&lt;/h3&gt;
&lt;p&gt;在目前的2D社交媒体中，用户只能通过图片和视频来分享和体验内容。虽然这种方式能够有效地传达信息，但其在沉浸感和真实性上无法与3D体验相抗衡。然而，在空间计算的时代，我们可以展现三维模型、具备深度感知的三维图片和视频，甚至是可以完全替代用户现实环境的3D场景。这些新的媒体形式并不会取代旧的媒体，而是将与之共存，共同丰富我们的媒体生态。&lt;/p&gt;

&lt;h3 id=&quot;破除互动性的限制&quot;&gt;破除互动性的限制&lt;/h3&gt;
&lt;p&gt;2D社交媒体的互动性主要表现为文字评论、点赞等较为单一的形式。然而，在3D的AR/VR环境中，用户可以通过更直观和丰富的方式来与内容和其他用户进行互动，比如直接在3D环境中移动、操控物体、进行手势交互等。&lt;/p&gt;

&lt;h3 id=&quot;开拓创作空间的广阔&quot;&gt;开拓创作空间的广阔&lt;/h3&gt;
&lt;p&gt;在2D环境中，创作者的表达方式被限制在一个二维的平面内。然而，在3D环境中，他们能够创造出更为丰富、更立体的内容。这意味着TikTok可能从一个“短视频”平台转变为一个“短体验（Short-form experiences）”平台。创作者们将不再受限于固定尺寸的屏幕，而是能够在一个三维空间内创作。他们可以选择在这个空间内只展示二维内容，或是进行二维与三维内容的混搭，或者是完全展示纯三维的场景。在纯三维场景中，创作者们还可以选择是否用虚拟内容完全替代用户的现实环境，或是将虚拟内容融入到用户的现实环境中。&lt;/p&gt;

&lt;p&gt;对于消费者来说，现有的TikTok通过将视频和音乐结合，已经能够为用户提供即时的乐趣。然而在未来，TikTok可以通过各种虚拟场景，将用户带入”短体验”中，为用户带来前所未有的多巴胺刺激。&lt;/p&gt;

&lt;p&gt;对于创作者来说，这个三维创作空间将会吸引更多的游戏开发者和建模师参与其中。制作“短体验”的成本将远远低于开发一款完整的游戏，这无疑将引发更多的创新和创意涌现。&lt;/p&gt;

&lt;h2 id=&quot;具体的用户体验会是什么样的&quot;&gt;具体的用户体验会是什么样的？&lt;/h2&gt;

&lt;p&gt;莱克独自坐在候机厅的长椅上，喧嚣的人群与他形成了鲜明的对比。他轻轻戴上AR眼镜，选择TikTok软件，立刻，TikTok以混合现实模式启动，仿佛打开了一个全新的世界的大门。&lt;/p&gt;

&lt;p&gt;一群虚拟的街头舞者出现在他面前狭窄的走道上，自由自地跳起最新的街舞挑战。在这个年代，拍摄3D视频和图片就如同曾经用手机拍摄2D视频一样简单，人人都可以做到。突然，其中一个男孩脚一滑、突然失去平衡，一头撞在坐在莱克对面的旅客身上，但那位旅客并未察觉，因为这些舞者只呈现在莱克的AR眼镜中。莱克微微一笑，虽然他对舞蹈没有太大的兴趣，但这种新鲜的体验仍然让他感到有趣。&lt;/p&gt;

&lt;p&gt;轻轻滑动手指，下一个体验已经浮现在他眼前。这是一条财经新闻，新闻主持人穿着蓝色西装，像是真人一样站在莱克的左前方。而莱特的右前方浮现出一个2D的股价图表，主持人正在报道某运动鞋公司股票暴跌的新闻。随后，股价图表立刻变为立体，一个新的坐标轴被加入其中——用户投诉数量。主持人一边旋转三维坐标，一边讲述近期的投诉情况。莱克可以清晰地看到随着时间的推移，用户投诉在逐步增加，而股价在持续下滑。在这个时代，制作这样的3D动画非常容易，入门级的3D场景设计软件如雨后春笋般崭露头角，这让许多创作者能够快速地制作3D内容。看到这，莱克心里庆幸自己并没有购买这家运动鞋公司的股票，然后他便继续向下划屏，进入下一个体验。&lt;/p&gt;

&lt;p&gt;此时，登机广播响起，莱克通过简单的手势，将TikTok的显示区域限定在他的右前方，然后起身向登机口走去。在排队等待登机的过程中，他可以一边观看右边的TikTok，一边关注前方队伍的动向。&lt;/p&gt;

&lt;p&gt;当莱克坐下，系上安全带后，他切换到了TikTok的沉浸模式。在这个模式下，他的周围的环境全部被替换成虚拟场景，就像是被传送到了另一个空间。他看到的第一件事是一只可爱的金毛在他前方的沙发上打滚。这是一个3D视频，莱克仿佛就坐在那个家庭的客厅里，仿佛可以伸手触摸到它毛茸茸的肚腩。如果是在混合现实模式下，TikTok将自动屏蔽客厅的背景，只把金毛放置在莱克现实的环境里。但是在沉浸模式下，莱克似乎被直接传送到了那个家庭的客厅，感受到他们和狗狗一起的快乐时光。&lt;/p&gt;

&lt;p&gt;下一个体验里，一个巨大的运动鞋出现在了莱克面前，在深黑的背景下，鞋子绚丽的旋转，犹如真实的在莱克眼前。伴随着一个铿锵有力的解说声音，鼓励莱克看向自己的脚，尝试AR试穿。原来这是一个3D广告，莱克觉得这双鞋看起来很眼熟，好像就是之前视频里滑倒的年轻人穿的那双。莱克并没有深思，而是继续滑屏进入下一个体验。&lt;/p&gt;

&lt;p&gt;…&lt;/p&gt;

&lt;h2 id=&quot;最后想法&quot;&gt;最后想法&lt;/h2&gt;

&lt;p&gt;值得一提的是，在今天的网络环境下，由于带宽的局限性，我们在实时下载大量的3D场景文件，来制作流畅的Feed时还面临着挑战。但随着5G时代的到来，一切都可能改变。在飞速的5G网络下，10-20MB的3D场景的实时下载将会变得游刃有余，就如同我们曾经无法想象在3G网络环境下TikTok能够流畅播放5-6MB的短视频一样。&lt;/p&gt;

&lt;p&gt;与此同时，3D模型和内容在未来会变得像今天的图片和视频一样寻常而易得。在未来，制作3D模型将变得与拍照一样简单 —— 事实上，现在我们就可以通过手机扫描物体来创建3D模型，这项技术在未来只会变得更加便捷和普及。&lt;/p&gt;

&lt;p&gt;与此类似，制作3D体验也将像剪辑视频一样入门简单但上限很高。用户可以用编辑器，很轻松布局3D物品、动画、3D视频、2D图片或者视频，并通过调整时间线来控制播放顺序、空间位置以及大小变化。&lt;/p&gt;

&lt;p&gt;再进一步，录制3D视频或拍摄3D照片也会像拍摄2D内容一样容易。一旦AR眼镜普及，任何人都可以用3D的方式轻松记录生活的精彩瞬间。这是一个充满无限可能和想象的新世界，我们期待着它的到来。&lt;/p&gt;

&lt;h2 id=&quot;结语&quot;&gt;结语&lt;/h2&gt;

&lt;p&gt;我在这里分享的所有观点都是我个人的思考。这些思考并不代表我所在的公司或我的雇主的立场，也绝不包含任何机密信息。我的目的仅仅是为了激发大家对未来可能的AR/VR应用的想象力和热情，通过分享我个人的理解和观察，帮助大家更好地理解这个充满未知和可能性的新领域。&lt;/p&gt;

&lt;p&gt;我完全意识到，尽管我尽力做出了深思熟虑的探索，但随着技术的发展和时间的推移，我所描绘的未来可能会被证明是错误的。站在未来往回看，我的很多观点或许会显得幼稚。但这并不影响我对这个探索过程的热爱和兴趣，我相信只有在试错中，我们才能发现新的可能性，创新出更好的解决方案。&lt;/p&gt;

&lt;p&gt;最后，我希望这个系列的文章能给你带来启发，同时也希望你能分享你的想法和观察。&lt;/p&gt;</content><author><name></name></author><summary type="html">前言</summary></entry><entry><title type="html">Reimagine Series Prologue</title><link href="http://localhost:4000/blogs/2023/summer/Reimagine_Prologue" rel="alternate" type="text/html" title="Reimagine Series Prologue" /><published>2023-06-09T07:40:23-07:00</published><updated>2023-06-09T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2023/summer/Reimagine_Prologue</id><content type="html" xml:base="http://localhost:4000/blogs/2023/summer/Reimagine_Prologue">&lt;h2 id=&quot;what-is-this-series-about&quot;&gt;What is this series about?&lt;/h2&gt;

&lt;p&gt;Welcome to the “Reimagine” series. In this series, I will select applications from the Top 100 in the app store and reimagine what they would be like in a three-dimensional space, as AR/VR software. We will no longer be confined to the two-dimensional interfaces of mobile phones and computers, but shift our focus towards the more tangible and realistic three-dimensional space provided by AR/VR.&lt;/p&gt;

&lt;h2 id=&quot;why-do-i-write-this-series&quot;&gt;Why do I write this series?&lt;/h2&gt;

&lt;p&gt;We can make two basic assumptions about the future of technology:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;People desire more natural and realistic ways of interaction.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Looking back over the past decade or so, it’s clear to see that from the text messages and emails of the 2G era to the pictures and voice messages of the 3G era, and then to the short videos and live broadcasts of the 4G era, the carriers of information flow are becoming increasingly enriched, presenting in ways that are closer to the real world.&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;As three-dimensional beings, humans naturally understand and accept three-dimensional content.&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Our everyday lives take place in three-dimensional space. In other words, we are inherently accustomed to interacting in three dimensions, not confined to two-dimensional screens.&lt;/p&gt;

&lt;p&gt;Based on these two assumptions, we have reason to believe that spatial computing brought about by AR/VR will be the next growth point in human technology.&lt;/p&gt;

&lt;p&gt;However, I found that many people’s thinking is still confined to a two-dimensional plane, and it’s hard to imagine what WeChat or Taobao would be like in three dimensions. Yet, the software on our phones satisfies various daily needs, which will still exist in the era of spatial computing. This implies that these apps on our phones need to be redesigned and implemented in a three-dimensional environment.&lt;/p&gt;

&lt;p&gt;Through this series, I hope to explore with you the possibilities of this new world and envision the various stunning applications that may appear in the future.&lt;/p&gt;

&lt;h2 id=&quot;how-will-i-appraoch-it&quot;&gt;How will I appraoch it?&lt;/h2&gt;

&lt;p&gt;To make this series more meaningful, I’ll undertake the following steps in my exploration:&lt;/p&gt;

&lt;p&gt;Firstly, we need to set up an ideal environment. Assume we already have an ideal AR headset: a comprehensive, smooth, and convenient device that seamlessly integrates virtual information into the real world, providing users with unprecedented interactive experiences.&lt;/p&gt;

&lt;p&gt;Under this premise, I’ll select a series of killer apps from the app store based on their ranking. These apps may include social, shopping, education, etc., all of which are favored by a broad user base due to their effectiveness in solving certain user needs.&lt;/p&gt;

&lt;p&gt;Then, I’ll apply the following three-step analysis to these apps:&lt;/p&gt;

&lt;ol&gt;
  &lt;li&gt;Who is it solving what problem for?&lt;/li&gt;
  &lt;li&gt;Why can AR/VR solve this problem better?&lt;/li&gt;
  &lt;li&gt;What will the specific user experience be like?&lt;/li&gt;
&lt;/ol&gt;

&lt;p&gt;Through these steps, I hope to spark our thinking about spatial computing and open our imagination to future technologies.&lt;/p&gt;

&lt;h2 id=&quot;closing&quot;&gt;Closing&lt;/h2&gt;

&lt;p&gt;At the start of this journey, I want to make it clear: all the views I share here are my own. These thoughts do not represent the positions of my company or employer, nor do they contain any confidential information. My sole aim is to stimulate imagination and enthusiasm for potential AR/VR applications, helping everyone better understand this new field full of unknowns and possibilities through my personal insights and observations.&lt;/p&gt;

&lt;p&gt;I am fully aware that although I’ve made thoughtful explorations, with technological advancements and the passage of time, my depiction of the future might prove incorrect. Looking back from the future, many of my views may seem naive. But that won’t dampen my passion and interest in this exploration. I believe that only through trial and error can we discover new possibilities and innovate better solutions.&lt;/p&gt;

&lt;p&gt;Finally, I hope this series will inspire you, and I also look forward to your sharing of ideas and observations.&lt;/p&gt;</content><author><name></name></author><summary type="html">What is this series about?</summary></entry><entry><title type="html">Vrlingo</title><link href="http://localhost:4000/projects/2022/fall/Vrlingo" rel="alternate" type="text/html" title="Vrlingo" /><published>2023-01-13T21:40:23-08:00</published><updated>2023-01-13T21:40:23-08:00</updated><id>http://localhost:4000/projects/2022/fall/Vrlingo</id><content type="html" xml:base="http://localhost:4000/projects/2022/fall/Vrlingo">&lt;p&gt;Have you ever tried speaking a new language? It’s hard right? To make it worse, there’s few opportunities to practice. You need to find a fluent speaker to practice with or spend a lot of money to hire a tutor. That’s why we’re dedicated to providing a platform for everyone to practice speaking new languages for free.&lt;/p&gt;

&lt;p&gt;We are a language learning community to help you practice with other people in virtual worlds. We have introduced a credit system, called TBucks, which allows you to gain credits by helping others practice speaking in your native language. You can then use those credits to practice a new language you want to learn.&lt;/p&gt;

&lt;p&gt;You can try it out &lt;a href=&quot;https://github.com/SCP650/Vrlingo-New&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/mgdhqsrxNBY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;free-language-practice&quot;&gt;Free Language Practice&lt;/h2&gt;

&lt;p&gt;We strive to create a community where each user not only learns a new language, but also acts as a teacher to assist others with their native language. After specifying the languages they know and wish to learn, we provide a live map of all current instances where those specified languages are being spoken. Users can then choose to join as a teacher to earn TBucks or as a student to spend them. This fosters a language learning community and allows for free practice opportunities for speaking.&lt;/p&gt;

&lt;h2 id=&quot;why-vr&quot;&gt;Why VR?&lt;/h2&gt;

&lt;p&gt;VR allows us to build situations and an immersive environment for role-playing.&lt;/p&gt;

&lt;p&gt;For example, we built a restaurant world with many interactive props. We noticed that many Mandarin learners will pretend to order food, talk about their favorite dishes, or try to rob the restaurant while speaking in Mandarin. This makes the language speaking fun and mimics real-life situations.&lt;/p&gt;

&lt;p&gt;To take it a step further, we made contextually-driven prompts that provide topics to talk about. For example, when the user picks up some money in the VR restaurant, they will see “Buy some food with money.” This invites a conversation between the user holding the money and other users in the VR restaurant.&lt;/p&gt;

&lt;h2 id=&quot;monetization&quot;&gt;Monetization&lt;/h2&gt;

&lt;p&gt;There are three ways we could monetize in the future.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;
    &lt;p&gt;Ads: We can place immersive ads in the VR world. For example, a branded soda in the restaurant world, or make branded coffee shop as a world for users to practice speaking in.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Professional Tutors: Some users may appreciate professional tutors such as 1:1 lessons in addition to the free community support. Hence, we can offer classes and take a percentage of the fee.&lt;/p&gt;
  &lt;/li&gt;
  &lt;li&gt;
    &lt;p&gt;Subscriptions: We can offer a monthly subscription where users can gain additional features such as show hints to pronounce when grabbing objects, special status icons, and remove ads.&lt;/p&gt;
  &lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;We use Unity and XR Interaction Toolkit to create the app. The multiplayer networking, VOIP, synced variables and avatar sync are created using Normcore.&lt;/p&gt;

&lt;p&gt;We made this app in 2 days and won the Best Shared World Experience Award at MIT Reality Hack!&lt;/p&gt;

&lt;p&gt;PS: We created the icon of this page using Stable Diffusion.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><category term="Product_Project_Management" /><summary type="html">Have you ever tried speaking a new language? It’s hard right? To make it worse, there’s few opportunities to practice. You need to find a fluent speaker to practice with or spend a lot of money to hire a tutor. That’s why we’re dedicated to providing a platform for everyone to practice speaking new languages for free.</summary></entry><entry><title type="html">Match-3 Siege</title><link href="http://localhost:4000/projects/2022/winter/Match3Shooter" rel="alternate" type="text/html" title="Match-3 Siege" /><published>2022-12-30T01:40:23-08:00</published><updated>2022-12-30T01:40:23-08:00</updated><id>http://localhost:4000/projects/2022/winter/Match3Shooter</id><content type="html" xml:base="http://localhost:4000/projects/2022/winter/Match3Shooter">&lt;p&gt;What would Candy Crush + Call of Duty look like?&lt;/p&gt;

&lt;p&gt;Many innovations in video games come from blending genres – what happens if we combine the most popular genre on mobile (match-3) with the most popular genre on PC (shooter)?&lt;/p&gt;

&lt;p&gt;Introducing Match-3 Siege! You can download the demo &lt;a href=&quot;https://github.com/SCP650/Match3Shooter-Unreal5/releases/tag/v1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;h2 id=&quot;how-does-it-work&quot;&gt;How does it work?&lt;/h2&gt;

&lt;p&gt;In this game, you’ll be part of either the attacking or defending team. The attackers must destroy all of the colored spheres on the field, while the defenders must protect them.&lt;/p&gt;

&lt;p&gt;To destroy the spheres, attackers must shoot and match three or more of the same color, while fighting against the defenders’ fire.&lt;/p&gt;

&lt;p&gt;To defend the spheres, the defenders need to find and guard strategic positions to prevent match-3 combinations from occurring (e.g. blue blue red blue) and insert new spheres into the field every 30 seconds to make it harder for the attackers (e.g. blue yellow blue red blue).&lt;/p&gt;

&lt;p&gt;If the attackers succeed in destroying all the spheres before time runs out, they win. If any spheres remain, the defenders win.&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/saNxsox4Wz8&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;p&gt;The video above is a single-player demo I made for this idea. Although it’s against AI, you can already feel the intense and fast-paced adrenaline rush caused by constantly switching between playing match-3 and fighting against enemies.&lt;/p&gt;

&lt;p&gt;I think this game mode could be added alongside team deathmatch, search and destroy, and battle royal. Let me know what you think!&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This game was made independently by me using Unreal Engine 5.1 and C++. It is my first time making Unreal games with C++ and it has been a great learning experience. I used free 3D and animation assets and implemented the following features:&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Animation blending using blendspace&lt;/li&gt;
  &lt;li&gt;Input mapping for locomotion&lt;/li&gt;
  &lt;li&gt;AI behavior tree and blackboard for enemies&lt;/li&gt;
  &lt;li&gt;Integration of particles and sound for shooting&lt;/li&gt;
  &lt;li&gt;Lose and win conditions&lt;/li&gt;
  &lt;li&gt;Match-3 system with chaos destruction.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;PS: I created the icon of this page using Stable Diffusion and the text of this page using ChatGPT&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="Game_Development" /><summary type="html">What would Candy Crush + Call of Duty look like?</summary></entry><entry><title type="html">A Second Person VR Game</title><link href="http://localhost:4000/projects/2022/fall/SecondPerson" rel="alternate" type="text/html" title="A Second Person VR Game" /><published>2022-11-26T21:40:23-08:00</published><updated>2022-11-26T21:40:23-08:00</updated><id>http://localhost:4000/projects/2022/fall/SecondPerson</id><content type="html" xml:base="http://localhost:4000/projects/2022/fall/SecondPerson">&lt;p&gt;Have you ever played a second-person multiplayer VR shooting game? Me neither, so I made one over the Thanksgiving break.&lt;/p&gt;

&lt;p&gt;Well, we first need to figure out what is a second-person game.&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;In a first-person game, you control yourself through your own perspective – I’m going left, I’m going right.&lt;/li&gt;
  &lt;li&gt;In a third-person game, you control yourself through an external perspective – They go left, they go right.&lt;/li&gt;
  &lt;li&gt;So naturally, in a second-person game, you control yourself through the perspective of another character – You go left, you go right.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;In this game, there are two players and two guns on the table. The goal is very simple: whoever picks up the gun and shoots the other player first wins. But there’s one catch: it’s a second-person game – you are controlling yourself through the perspective of your opponent.&lt;/p&gt;

&lt;p&gt;What does that actually mean? In this clip, it may look like a normal first-person game but you are not controlling yourself – if you raise your VR controller, the character in front of you will raise their hand, not the character you are in the perspective of.&lt;/p&gt;

&lt;p&gt;Originally I thought this would be as if I’m controlling myself through a mirror, but in reality, it is much worse …&lt;/p&gt;

&lt;p&gt;You can try it out &lt;a href=&quot;https://github.com/SCP650/SecondPersonVRGame/releases/tag/v1&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/fCkHKXO1GiU&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-true-second-person-vr-games-are-bad&quot;&gt;1. True Second Person VR Games are BAD&lt;/h3&gt;

&lt;p&gt;To make a true second-person game meant you cannot control the camera, since the camera is technically on the head of another character who you don’t control – you just happen to look out from their perspective.&lt;/p&gt;

&lt;p&gt;What that means for VR is that when you move your head to look right, the view you are seeing is not moving, instead, you have to ask the other player to move their head to look right so that you can see what’s on the right.&lt;/p&gt;

&lt;p&gt;This also means if the other player moves their head randomly, their view is not going to be impacted (since your head is not moving), but your view will move randomly causing vertigo and dizziness.&lt;/p&gt;

&lt;h3 id=&quot;2-a-compatitive-game-become-cooperative-quickly&quot;&gt;2. A Compatitive Game Become Cooperative Quickly&lt;/h3&gt;

&lt;p&gt;Even though the goal of the game is to kill the other player, we quickly realize it’s not playable if we don’t cooperate.&lt;/p&gt;

&lt;p&gt;In the video, you can see we struggle to get our own character inside our view. This is because:&lt;/p&gt;
&lt;ul&gt;
  &lt;li&gt;Player A can only see stuff from Player B’s perspective. So A needs to give B instructions to adjust A’s view.&lt;/li&gt;
  &lt;li&gt;But B doesn’t know what A is seeing (B can only see A’s perspective), so B doesn’t know many degrees they need to turn.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;Hence we end up spending the majority of the play time telling the other party how to move their head – “Can you look right?”,”Can you look left?” – which I guess makes this a successful second-person game..? but not an enjoyable game.&lt;/p&gt;

&lt;h3 id=&quot;3-player-will-instinctively-move-their-head-to-adjust-the-camera&quot;&gt;3. Player Will Instinctively Move Their Head to Adjust the Camera&lt;/h3&gt;

&lt;p&gt;One added difficulty I wasn’t expecting before was that when A wants to look left, A will not only give instructions to B to look left but also instinctively move their head to the left – this means it’s also disrupting B’s view.&lt;/p&gt;

&lt;p&gt;And then B’s view is disrupted, B will instinctively move their head in the opposite direction, further disrupting A’s view. This would create a compound effect what gets worse very quickly…&lt;/p&gt;

&lt;h2 id=&quot;some-thoughts&quot;&gt;Some Thoughts…&lt;/h2&gt;

&lt;p&gt;I started this project knowing a second-person VR game would be a bad idea. But my main goal is to figure out how to network a VR game, and I did end up with a multiplayer VR game so I consider this a success!&lt;/p&gt;

&lt;p&gt;Nonetheless, let this serve as a lesson to whoever comes after me that wants to make a second-person VR game. Unless you want to make major sacrifices (like in Trover Saves the Universe), a true second-person game is not a good idea.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with &lt;a href=&quot;https://assetstore.unity.com/packages/3d/props/interior/polygon-dining-room-199435&quot;&gt;free 3D assets&lt;/a&gt; from Asset Store, the Oculus Plugin, and the BNG Interaction Framework. It is networked using Normcore. This app is made in Unity with Oculus Quest 2. It can also run on a windows machine with Oculus Link.&lt;/p&gt;

&lt;p&gt;PS: I created the icon of this page using text to image AI.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever played a second-person multiplayer VR shooting game? Me neither, so I made one over the Thanksgiving break.</summary></entry><entry><title type="html">Earf</title><link href="http://localhost:4000/projects/2022/summer/earf" rel="alternate" type="text/html" title="Earf" /><published>2022-08-20T22:40:23-07:00</published><updated>2022-08-20T22:40:23-07:00</updated><id>http://localhost:4000/projects/2022/summer/earf</id><content type="html" xml:base="http://localhost:4000/projects/2022/summer/earf">&lt;p&gt;Enjoying pictures of Earth? Why not try Earf – the AR Planet Generator!&lt;/p&gt;

&lt;ul&gt;
  &lt;li&gt;Tap on the screen to generate gorgeous planets&lt;/li&gt;
  &lt;li&gt;Move your device around to view them up close&lt;/li&gt;
  &lt;li&gt;Simulate physics to see planets collide!&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;To download the full app: &lt;a href=&quot;https://apps.apple.com/us/app/earf-ar-planets/id1641207075&quot;&gt;Apple App Store&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/UuTJdkB-cac&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This app is made with Unity using AR Foundation. Each terrains is procedurally genearted using multiple layers of perlin noise. A custom shader is made to coloring the planets with random gradients based on the distance of each vertex to the center. It simulations physically (sorta) accurate gravitational force. And also has a random name generator to give your pet planets names!&lt;/p&gt;

&lt;h2 id=&quot;how-was-it-made&quot;&gt;How was it made?&lt;/h2&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p1.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p1.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Starting with a blob&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p2.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p2.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Make it round&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p3.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p3.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Sprinkle some perlin noise&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p4.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p4.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Clamp the noise to make ocean surface&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p5.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p5.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the mountains with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p6.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p6.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the planet with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p7.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p7.png&quot; alt=&quot;&quot; class=&quot; w-75 text-center &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Paint the ocean depths with a shader&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/earf/p8.png&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/earf/p8.png&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Done!&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><summary type="html">Enjoying pictures of Earth? Why not try Earf – the AR Planet Generator!</summary></entry><entry><title type="html">Siggraph 2022 Day 2</title><link href="http://localhost:4000/blogs/2022/summer/Siggraph22_Part2" rel="alternate" type="text/html" title="Siggraph 2022 Day 2" /><published>2022-08-12T07:40:23-07:00</published><updated>2022-08-12T07:40:23-07:00</updated><id>http://localhost:4000/blogs/2022/summer/Siggraph22_Part2</id><content type="html" xml:base="http://localhost:4000/blogs/2022/summer/Siggraph22_Part2">&lt;p&gt;Hi all, welcome to my Siggraph series where I post the TLDR of the most memorable stuff I see at the conference.&lt;/p&gt;

&lt;p&gt;This started as some rambling notes I took during the conference, and I just cleaned it up a bit. It may not be the most accurate (especially the technical papers) but feel free to take a look!&lt;/p&gt;

&lt;h2 id=&quot;day-2-events-ive-attended&quot;&gt;Day 2 Events I’ve attended:&lt;/h2&gt;

&lt;ul&gt;
  &lt;li&gt;Tech Papers: &lt;strong&gt;Ray Tracing &amp;amp; Monte Carlo Methods&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: &lt;strong&gt;Emerging Technologies&lt;/strong&gt; + &lt;strong&gt;Immersive Pavilion&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Presentation: &lt;strong&gt;Advances in Real-Time Rendering in Games: Part II&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: Companies&lt;/li&gt;
  &lt;li&gt;Unreal Talk: &lt;strong&gt;Animating In-Engine - Real-Time Production Workflows&lt;/strong&gt;&lt;/li&gt;
  &lt;li&gt;Expo: Appy Hour&lt;/li&gt;
&lt;/ul&gt;

&lt;h2 id=&quot;highlights&quot;&gt;Highlights&lt;/h2&gt;
&lt;hr /&gt;
&lt;h2 id=&quot;immersive-pavilion&quot;&gt;Immersive Pavilion&lt;/h2&gt;

&lt;h3 id=&quot;a-vr-locomotion-method-that-kinda-solves-motion-sickness&quot;&gt;A VR Locomotion Method that (kinda) Solves Motion Sickness&lt;/h3&gt;

&lt;p&gt;What is it: The demo is called “&lt;strong&gt;HyperJumping&lt;/strong&gt;” by Bernhard E. Riecke. It has two parts: lean based locomotion and hyperjump.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Lean-Based Locomotion&lt;/strong&gt;: instead of pushing a thumbstick, you have to physically lean forward to move forward. Since your body is actually leaning forward as your visual changes, it’s less dizzy.&lt;/p&gt;

&lt;p&gt;&lt;strong&gt;Hyperjump&lt;/strong&gt;: when the user reached a max speed, it will automatically teleport the user for a segment, and then the user will continue to travel at max speed.&lt;/p&gt;

&lt;p&gt;Use case: if a person wants to travel long distances in VR, they don’t want to use smooth locomotion (which is dizzy) and don’t want to teleport (since they will miss the view), they will use lean-based movement + hyperjump.&lt;/p&gt;

&lt;p&gt;Sebastian’s Review: conceptually this is a very interesting idea — combining physical movement with smooth locomotion and teleportation, but in practice, when I tried the demo the Hyperjump(teleportation) feels scary — I would suddenly teleport to another place without any warning. I really like the idea and think further exploration is required.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;expo-companies&quot;&gt;Expo: Companies&lt;/h2&gt;

&lt;p&gt;I tried a ton of demos at the expo, these are the top most memorable three&lt;/p&gt;

&lt;h3 id=&quot;omniverse&quot;&gt;Omniverse&lt;/h3&gt;

&lt;p&gt;I was really really shocked when I saw a live demo of Nvidia’s Omniverse. The demo showcases a game development workflow of 3 people, using Maya, Unreal and Adobe Substance 3D respectively on their own laptops.&lt;/p&gt;

&lt;p&gt;With Omniverse, everything is synced in real-time bi-directionally: if a new car is added in Unreal’s scene, it will be reflected in Maya and Substance 3D instantly. Then the artist in Substance 3D will pain the car, the texture will show up instantly in Unreal and Maya.&lt;/p&gt;

&lt;p&gt;This workflow is very magical and can drastically increase a team’s collaboration efficiency. Under the hood, everything works because they are using Disney’s new and open Universal Scene Description (USD) format. 
&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
      &lt;a href=&quot; /images/projects/siggraph22/omniverse.jpg&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
    
        &lt;img src=&quot; /images/projects/siggraph22/omniverse.jpg&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Omniverse Integarting 3 Different Workflow&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;h3 id=&quot;oppo-air-glass&quot;&gt;Oppo Air glass&lt;/h3&gt;

&lt;p&gt;I tried Oppo’s Air glass, which is an extension screen you can attach to any glasses. It can display information like weather, messages and live translation.&lt;/p&gt;

&lt;p&gt;In theory, it sounds very good, but when I tried it in practice, I need to constantly change my eye focus between the glass and the person I’m talking to in real life which result in fatigue very quickly. The major issue is that the info is not spatialized. Having a screen that constantly displays irrelevant info is also distracting. It’s a good tech demo but would need a lot more improvement to be consume-ready.&lt;/p&gt;

&lt;h3 id=&quot;stretchsense-gloves&quot;&gt;StretchSense Gloves&lt;/h3&gt;

&lt;p&gt;A current problem with optical hand tracking is that if the hand is obstructed, it can’t be tracked. StretchSense gloves managed to imbed a lot of senses in a normal looking glove, and send rotational information from each joint to their software, which will then convert the data to Unity/Unreal supported format.&lt;/p&gt;

&lt;p&gt;On the update, the glove feels comfortable and does hand tracking very feel. But it comes at a hefty~$5k. It’s a good mocap solution and for research use, but probably not for consumer tech.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;animating-in-engine---real-time-production-workflows&quot;&gt;Animating In-Engine - Real-Time Production Workflows&lt;/h2&gt;

&lt;p&gt;The folks at Epic Games showcased how you can create animations in-engineer without the need for professional software like Maya.&lt;/p&gt;

&lt;p&gt;In the demo, the speaker is able to copy parts of one animation (jumping) and paste them to another animation (moving forward), the final result is a person hopping forward.&lt;/p&gt;

&lt;p&gt;This is very impressive in that programmers can just download a few free mocap assets, and cut and paste different portions to combine to the desired motions. This makes prototyping ideas and making short video clips so much faster.&lt;/p&gt;

&lt;hr /&gt;
&lt;h2 id=&quot;tech-papers-ray-tracing--monte-carlo-methods&quot;&gt;Tech Papers: Ray Tracing &amp;amp; Monte Carlo Methods&lt;/h2&gt;

&lt;h3 id=&quot;r2e2-low-latency-path-tracing-of-terabyte-scale-scenes-using-thousands-of-cloud-cpus&quot;&gt;R2E2: Low-latency Path Tracing of Terabyte-scale Scenes Using Thousands of Cloud CPUs&lt;/h3&gt;

&lt;p&gt;Problem: if you need to render terabyte-scale scenes, and you don’t happen to own a super computer… you are out of luck&lt;/p&gt;

&lt;p&gt;Solution: Why now rent thousands of CPUs to run at the same time? That’s what the author did, live, at the presentation — he rented thousands of AWS E3 instances and rendered a path-traced picture of a terabyte scene in ~1 mins.&lt;/p&gt;

&lt;h3 id=&quot;generalized-resampled-importance-sampling-foundations-of-restir&quot;&gt;Generalized Resampled Importance Sampling: Foundations of ReSTIR&lt;/h3&gt;

&lt;p&gt;Problem: Traditional ray tracing is very costly and slow, it also requires multiple passes to generate a good image.&lt;/p&gt;

&lt;p&gt;Context: The ReSTIR approach - it reuses samples from previous frames to improve path distributions across frames (temporal reuse) and resample between this and nearby pixels (spatial reuse)&lt;/p&gt;

&lt;p&gt;Solution: Through some cool math and magic that I don’t fully understand, the author generalized the ReSTIR approach to work on any domains without the need to know the exact PDF&lt;/p&gt;

&lt;h3 id=&quot;regression-based-monte-carlo-integration&quot;&gt;Regression-based Monte Carlo Integration&lt;/h3&gt;

&lt;p&gt;Problem: Solving integration is a hard but common problem in graphics, e.g. to light up one pixel properly, we need to calculate the integral of all the light paths connecting the pixel to the light source.&lt;/p&gt;

&lt;p&gt;Context: Currently we mostly use Monte Carlo Integration, which randomly samples the function and averages those values to estimate the integral. However, this is only an estimate and has errors.&lt;/p&gt;

&lt;p&gt;Solutions: the author proposes a new estimator using regression function + residual, this estimator is provable better than Monte Carlo. Because in the worst case, the regression will be a constant, which will produce the same result as Monte Carlo.&lt;/p&gt;</content><author><name></name></author><summary type="html">Hi all, welcome to my Siggraph series where I post the TLDR of the most memorable stuff I see at the conference.</summary></entry><entry><title type="html">Lightly Heavy</title><link href="http://localhost:4000/projects/2022/spring/Heavy" rel="alternate" type="text/html" title="Lightly Heavy" /><published>2022-02-04T16:40:23-08:00</published><updated>2022-02-04T16:40:23-08:00</updated><id>http://localhost:4000/projects/2022/spring/Heavy</id><content type="html" xml:base="http://localhost:4000/projects/2022/spring/Heavy">&lt;p&gt;Have you ever wondered what it feels like to fly in space? Well, now you can. This virtual reality game allows you to simulate space combat like in the Ender’s Game anytime and anywhere.&lt;/p&gt;

&lt;p&gt;This app is built in 36 hours independently by Sebastian Yang with original music composed by William Ozeas. It is developed for HoyaHack and won &lt;strong&gt;the U.S. Space Force Award&lt;/strong&gt;. You can download it and try it out &lt;a href=&quot;https://github.com/SCP650/LightlyHeavy-UnityVR/releases&quot;&gt;here&lt;/a&gt;.&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/VyIAN2M-e8o&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-lock-rotations-and-lower-the-speed&quot;&gt;1. Lock Rotations and Lower the Speed!&lt;/h3&gt;

&lt;p&gt;I always thought continuous movement and jumping up and down in VR are the dizziest. I was wrong. When I first develop the movement mechanics, I didn’t lock rotation meaning if the player used the booster on their right hand, instead of going straight forward, they will start spinning like crazy while moving forward.&lt;/p&gt;

&lt;p&gt;The first few seconds were fun - as the speed is still slow. But afterward, the dizziness became unbearable. The takeaway here is to lower the speed to avoid dizziness, especially for beginners.&lt;/p&gt;

&lt;h3 id=&quot;2-prioritize-before-acting&quot;&gt;2. Prioritize Before Acting&lt;/h3&gt;

&lt;p&gt;There are two major components to this project: the tutorial area (spaceship) and the combat zone (giant sphere). Most developers will probably start with the combat zone since it’s the bulk of the game. However, I decided to start with the tutorial area.&lt;/p&gt;

&lt;p&gt;This is because the combat zone is essentially the tutorial areas with procedural generation – both shared the same movement systems and enemies. If I don’t have time to develop a good pro-gen system, the player could still learn how to play the game and explore a single level - the spaceship. However, if I started with the combat zone and the pro-gen is not working, the player would not know what to do and killed by enemies instantly.&lt;/p&gt;

&lt;p&gt;Even though I managed to finish the pro-gen systems, this prioritization could’ve saved my project.&lt;/p&gt;

&lt;h3 id=&quot;3-its-dangerous-to-go-alone&quot;&gt;3. It’s Dangerous to Go Alone!&lt;/h3&gt;

&lt;p&gt;I intentionally made this a solo project because I want to test my ability. Even though I was happy with the result, it was not fun to do everything by myself. I would prefer the collaboration and delegation I did in &lt;a href=&quot;http://localhost:4000/projects/2021/fall/Haunted&quot;&gt;Haunted&lt;/a&gt;. If possible, don’t go alone!&lt;/p&gt;

&lt;h2 id=&quot;some-thoughts&quot;&gt;Some Thoughts…&lt;/h2&gt;

&lt;p&gt;I’m very proud of this project, especially with the execution. In 36 hours, I managed to develop a movement mechanic in zero-g, an extendable enemy class system, and a procedural generation system for levels. I also managed to design a tutorial area with 3D assets, a combat zone in Blender, and multiple GUI components. And everything was done in a virtual reality setting.&lt;/p&gt;

&lt;p&gt;This was by no means a perfect execution – the enemy AI needs a lot more work and there’s a lack of more enemy and weapon types. But I’m very pleased with what I ended up with - a replayable space combat VR game that is actually fun to play.&lt;/p&gt;

&lt;p&gt;Moreover, it also (kinda) fulfilled my childhood dream - going to space. The Ender’s Game is one of my favorite novel series and movies. Seeing the fight in a giant sphere in space really inspired and motivated me. (heck I even applied to many top aerospace engineering programs during college applications). This project intersects my high school passion in space with my college passion in the metaverse. So I’m really glad I got to do this.&lt;/p&gt;

&lt;p&gt;Finally, this project is also a testament to my growth. When I first started three years ago, I would’ve never thought I can pull something like this off. During the past three years, I’ve tried a lot, failed a lot, learned a lot, and kept repeating. It is truly pleasing to see oneself grow.&lt;/p&gt;

&lt;p&gt;Now that my college life is over, a new journey awaits.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with &lt;a href=&quot;https://assetstore.unity.com/packages/3d/environments/sci-fi/sci-fi-styled-modular-pack-82913&quot;&gt;free 3D assets&lt;/a&gt; from Asset Store, free skyboxes, the Oculus Plugin and the BNG Interaction Framework. This app is made in Unity with Oculus Quest 2. It can run on a windows machine with Oculus Link.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever wondered what it feels like to fly in space? Well, now you can. This virtual reality game allows you to simulate space combat like in the Ender’s Game anytime and anywhere.</summary></entry><entry><title type="html">Game Creation Society</title><link href="http://localhost:4000/projects/2021/fall/GCS" rel="alternate" type="text/html" title="Game Creation Society" /><published>2022-01-23T16:40:23-08:00</published><updated>2022-01-23T16:40:23-08:00</updated><id>http://localhost:4000/projects/2021/fall/GCS</id><content type="html" xml:base="http://localhost:4000/projects/2021/fall/GCS">&lt;p&gt;The Game Creation Society is a student-led game development club at Carnegie Mellon. I’m honored to be elected as President of the Game Creation Society in 2020. In the fall semester alone, we have 100+ active members making 9 games in parallel.&lt;/p&gt;

&lt;p&gt;I joined this club in my freshman year not knowing anything about game development. But my time at GCS has been exceptional, I learned so much, made so many games, and met so many cool people. Here are some notable accomplishments we achieved during my time as President.&lt;/p&gt;

&lt;h2 id=&quot;accomplishments&quot;&gt;Accomplishments&lt;/h2&gt;
&lt;ul&gt;
  &lt;li&gt;&lt;strong&gt;Attendence&lt;/strong&gt;:We had a record turnout this year. At the start of the semester, we started with 250+ people attending our info session and booth showing interest to join and end up with 100+ people actually attending. By the end of the semester, we successfully released 8 games at Hunt Library.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;VR&lt;/strong&gt;: We purchased our first VR headset as a club and made two VR games in the semester after.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Unreal + Unity Stuco&lt;/strong&gt;: We offered the first Unreal Engine course at Carnegie Mellon and continued our existing Unity student-taught course.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Developer Fund&lt;/strong&gt;: We started a fund with a  few hundred dollars to support our developers&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;GCS Library&lt;/strong&gt;: We started a collection of assets that can be shared with all our members.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Discord&lt;/strong&gt;: Our Discord server continues to grow, now we have almost 600 members on the server.&lt;/li&gt;
  &lt;li&gt;&lt;strong&gt;Industry Talks&lt;/strong&gt;: We continued to host great industry talks.
    &lt;ul&gt;
      &lt;li&gt;We hosted an info session with &lt;em&gt;Epic Games&lt;/em&gt; for internship opportunity&lt;/li&gt;
      &lt;li&gt;We have &lt;em&gt;Rockstar Games&lt;/em&gt; gave the opening speech at our Pitch Fair.&lt;/li&gt;
      &lt;li&gt;We also invited alumni to join us, including the Tech Lead at &lt;em&gt;Unity&lt;/em&gt;, the Manager of Production at &lt;em&gt;PlayStation Studios&lt;/em&gt;, and the Lead Character Artist at &lt;em&gt;Heart Machine&lt;/em&gt;.&lt;/li&gt;
    &lt;/ul&gt;
  &lt;/li&gt;
  &lt;li&gt;Personally, during my three years at GCS, I’ve made six games including PC, Mobile and VR games.&lt;/li&gt;
&lt;/ul&gt;

&lt;p&gt;&lt;br /&gt;&lt;/p&gt;
&lt;div class=&quot;image-wrapper text-center&quot;&gt;
    
    &lt;a href=&quot;https://www.gamecreation.org&quot; title=&quot;&quot; target=&quot;_blank&quot;&gt;
      
        &lt;img src=&quot; /images/projects/GCS/website.png&quot; alt=&quot;&quot; class=&quot; w-100 &quot; /&gt;

    &lt;/a&gt;
    
        &lt;p&gt;Game Creation Society Website&lt;/p&gt;
    


&lt;/div&gt;

&lt;!-- source from https://superdevresources.com/image-caption-jekyll/ --&gt;

&lt;h2 id=&quot;to-gcs-members&quot;&gt;To GCS Members&lt;/h2&gt;

&lt;p&gt;I’m very honored to lead the club in the past year. I want to thank all the execs, team leads, alumni, and most of all, our members for giving me such a great experience.  I’ve no doubt the incoming execs will lead the club to the next great chapter, especially with the help of each and every one of you!&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="Software_Engineer" /><category term="Game_Development" /><category term="Product_Project_Management" /><summary type="html">The Game Creation Society is a student-led game development club at Carnegie Mellon. I’m honored to be elected as President of the Game Creation Society in 2020. In the fall semester alone, we have 100+ active members making 9 games in parallel.</summary></entry><entry><title type="html">Bat and Monitors</title><link href="http://localhost:4000/projects/2022/spring/Monitors" rel="alternate" type="text/html" title="Bat and Monitors" /><published>2022-01-22T16:40:23-08:00</published><updated>2022-01-22T16:40:23-08:00</updated><id>http://localhost:4000/projects/2022/spring/Monitors</id><content type="html" xml:base="http://localhost:4000/projects/2022/spring/Monitors">&lt;p&gt;Have you ever felt exhausted after a long day of work? Wanting to find a safe and private place to let it all out? Well, now you can. This application allows you to release all your stress and anger whenever you want and wherever you are. This is a rage room in virtual reality that gives you a baseball bat, and a lot of monitors.&lt;/p&gt;

&lt;p&gt;This app is built in 23 hours independently by Sebastian Yang for µHacks and it is the &lt;strong&gt;2nd place winner&lt;/strong&gt;. You can download it &lt;a href=&quot;https://github.com/SCP650/RageRoom-UnityVR/releases&quot;&gt;here&lt;/a&gt;&lt;/p&gt;

&lt;div class=&quot;iframe-container&quot;&gt;&lt;iframe src=&quot;https://www.youtube.com/embed/lErdRuNXWKY&quot; frameborder=&quot;0&quot; allow=&quot;accelerometer; autoplay; encrypted-media; gyroscope; picture-in-picture&quot; allowfullscreen=&quot;&quot;&gt;&lt;/iframe&gt;&lt;/div&gt;
&lt;p&gt;&lt;br /&gt;&lt;/p&gt;

&lt;h2 id=&quot;main-takeaways&quot;&gt;Main Takeaways&lt;/h2&gt;

&lt;h3 id=&quot;1-a-well-scoped-project-is-very-important-for-hackathons&quot;&gt;1. A Well-Scoped project is Very Important for Hackathons.&lt;/h3&gt;

&lt;p&gt;In my previous hackathons, my team and I always over-scope and end up staying up late or not finishing all we wanted to do. This time I pick a very small scope, I ended finished the majority of the content on Saturday morning and have an entire afternoon to play with procedural generation.&lt;/p&gt;

&lt;h3 id=&quot;2-cheat-the-eyes-with-sound-effects&quot;&gt;2. Cheat the Eyes with Sound Effects&lt;/h3&gt;

&lt;p&gt;To properly shatter an object, I’ll have to use Bender to cut up the meshes and textures into smaller pieces. That’s a lot of work considering the number of objects I have. Luckily, I notice that people care more about the moment they hit the object than the actual shattered pieces of objects – especially the pieces that are smaller.&lt;/p&gt;

&lt;p&gt;So I ended up using particle systems to “fake” shattering for smaller objects like cups and pen holders. Coupled with a satisfying sound effect, it managed to fool people into thinking they actually shattered it. But if they look closer, they will notice all the pieces are just cubes&lt;/p&gt;

&lt;h3 id=&quot;3-always-back-up&quot;&gt;3. Always Back Up&lt;/h3&gt;

&lt;p&gt;I tend to use GitHub to do version control, however, committing assets from the Unity Assets Store is against the guideline. So I have to gitignore all those assets to keep my Github repo public. During my development, I made a lot of extensions based on store assets – creating new prefabs, extending code, etc. I even edited some assets directly. However, these edits are not tracked by GitHub.&lt;/p&gt;

&lt;p&gt;I ended up accidentally deleting all the store assets when merging branches and staring at an empty scene halfway through development – luckily I recovered everything from the recycling bin and I have intermediate builds I could always submit.&lt;/p&gt;

&lt;p&gt;However, in the future, if I have to gitignore important assets. I should not only make intermediate builds but also make a local copy of the entire project at each milestone.&lt;/p&gt;

&lt;h2 id=&quot;under-the-hood&quot;&gt;Under the Hood&lt;/h2&gt;

&lt;p&gt;This is made independently by me with Unity’s free 3D Snaps Prototype assets, free skyboxes, the Oculus Plugin and the BNG Interaction Framework. This app is made in Unity with Oculus Quest 2. It can run on a windows machine AND standalone in an Oculus Quest 2 headset without the need of a PC.&lt;/p&gt;</content><author><name></name></author><category term="All" /><category term="AR_VR" /><category term="Software_Engineer" /><category term="Game_Development" /><summary type="html">Have you ever felt exhausted after a long day of work? Wanting to find a safe and private place to let it all out? Well, now you can. This application allows you to release all your stress and anger whenever you want and wherever you are. This is a rage room in virtual reality that gives you a baseball bat, and a lot of monitors.</summary></entry></feed>